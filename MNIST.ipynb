{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdfbb475",
   "metadata": {},
   "source": [
    "### Untargeted Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fb5b533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# Full Implementation with Attack Selection, Percentage Control, and Real Defense Methods\n",
    "# -------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "import math\n",
    "\n",
    "# -------------------\n",
    "# CNN Model\n",
    "# -------------------\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# -------------------\n",
    "# Data Partition (IID or non-IID using Dirichlet)\n",
    "# -------------------\n",
    "def partition_data(dataset, num_clients=100, alpha=0.5, iid=True):\n",
    "    if iid:\n",
    "        data_split = torch.utils.data.random_split(dataset, [len(dataset) // num_clients] * num_clients)\n",
    "    else:\n",
    "        labels = np.array(dataset.targets)\n",
    "        idx_by_class = [np.where(labels == i)[0] for i in range(10)]\n",
    "        data_split = [[] for _ in range(num_clients)]\n",
    "        for c, idx in enumerate(idx_by_class):\n",
    "            np.random.shuffle(idx)\n",
    "            proportions = np.random.dirichlet(np.repeat(alpha, num_clients))\n",
    "            proportions = (np.cumsum(proportions) * len(idx)).astype(int)[:-1]\n",
    "            splits = np.split(idx, proportions)\n",
    "            for client_id, split in enumerate(splits):\n",
    "                data_split[client_id].extend(split)\n",
    "        data_split = [Subset(dataset, idxs) for idxs in data_split]\n",
    "    return {i: DataLoader(data_split[i], batch_size=32, shuffle=True) for i in range(num_clients)}\n",
    "\n",
    "# -------------------\n",
    "# Attack Injection\n",
    "# -------------------\n",
    "malicious_client_ids = []\n",
    "\n",
    "def apply_label_flipping_attack(loader):\n",
    "    flip_map = {0:1, 1:0, 2:3, 3:2, 4:5, 5:4, 6:7, 7:6, 8:9, 9:8}\n",
    "    attacked = []\n",
    "    for x, y in loader:\n",
    "        y_flipped = torch.tensor([flip_map[int(label)] for label in y])\n",
    "        attacked.extend([(x[i], y_flipped[i]) for i in range(len(x))])\n",
    "    x_tensor = torch.stack([d[0] for d in attacked])\n",
    "    y_tensor = torch.tensor([d[1] for d in attacked])\n",
    "    return DataLoader(TensorDataset(x_tensor, y_tensor), batch_size=32, shuffle=True)\n",
    "\n",
    "def apply_feature_manipulation_attack(loader):\n",
    "    attacked = []\n",
    "    for x, y in loader:\n",
    "        # Stronger manipulation: add structured noise and masking\n",
    "        noise = torch.randn_like(x) * 0.7  # Increased noise factor\n",
    "        mask = (torch.rand_like(x) > 0.7).float()  # Random masking\n",
    "        x = x * mask + noise * (1 - mask)  # Apply noise and mask\n",
    "        attacked.extend([(x[i], y[i]) for i in range(len(x))])\n",
    "    x_tensor = torch.stack([d[0] for d in attacked])\n",
    "    y_tensor = torch.tensor([d[1] for d in attacked])\n",
    "    return DataLoader(TensorDataset(x_tensor, y_tensor), batch_size=32, shuffle=True)\n",
    "\n",
    "def apply_poisoning_attack(loader):\n",
    "    attacked = []\n",
    "    for x, y in loader:\n",
    "        # Complex poisoning: inject strong pixel shift and structured trigger pattern\n",
    "        x = torch.clamp(x + 0.3 * torch.ones_like(x), 0, 1)  # Shift intensities\n",
    "        trigger = torch.zeros_like(x)\n",
    "        trigger[:, :, -3:, -3:] = 1.0  # Add white square trigger at bottom-right\n",
    "        x = torch.clamp(x + trigger, 0, 1)\n",
    "        attacked.extend([(x[i], y[i]) for i in range(len(x))])\n",
    "    x_tensor = torch.stack([d[0] for d in attacked])\n",
    "    y_tensor = torch.tensor([d[1] for d in attacked])\n",
    "    return DataLoader(TensorDataset(x_tensor, y_tensor), batch_size=32, shuffle=True)\n",
    "\n",
    "# -------------------\n",
    "# Combined Attack Function\n",
    "# -------------------\n",
    "def apply_combined_attacks(client_data, attack_types, attack_ratios):\n",
    "    # Select malicious clients for each attack separately\n",
    "    attack_specific_ids = {}\n",
    "    total_clients = list(client_data.keys())\n",
    "    num_clients = len(total_clients)\n",
    "\n",
    "    for attack in attack_types:\n",
    "        count = int(num_clients * attack_ratios.get(attack, 0))\n",
    "        attack_specific_ids[attack] = random.sample(total_clients, count)\n",
    "\n",
    "    malicious_ids = list(set().union(*attack_specific_ids.values()))\n",
    "    global malicious_client_ids\n",
    "    malicious_client_ids = malicious_ids\n",
    "\n",
    "    for client_id in malicious_ids:\n",
    "        loader = client_data[client_id]\n",
    "        if 'label_flipping' in attack_types and client_id in attack_specific_ids.get('label_flipping', []):\n",
    "            loader = apply_label_flipping_attack(loader)\n",
    "        if 'feature_manipulation' in attack_types and client_id in attack_specific_ids.get('feature_manipulation', []):\n",
    "            loader = apply_feature_manipulation_attack(loader)\n",
    "        if 'poisoning' in attack_types and client_id in attack_specific_ids.get('poisoning', []):\n",
    "            loader = apply_poisoning_attack(loader)\n",
    "        client_data[client_id] = loader\n",
    "\n",
    "    return client_data\n",
    "\n",
    "# -------------------\n",
    "# Evaluation Function\n",
    "# -------------------\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            if isinstance(batch, (list, tuple)) and len(batch) == 2:\n",
    "                data, target = batch\n",
    "            else:\n",
    "                raise ValueError(\"Each batch in test_loader must be a (data, target) tuple.\")\n",
    "\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            total += target.size(0)\n",
    "            y_true.extend(target.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(10)))\n",
    "    tp = np.diag(cm)\n",
    "    fp = np.sum(cm, axis=0) - tp\n",
    "    fn = np.sum(cm, axis=1) - tp\n",
    "    tn = np.sum(cm) - (tp + fp + fn)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    fpr = np.mean(fp / (fp + tn + 1e-6))\n",
    "    tpr = np.mean(tp / (tp + fn + 1e-6))\n",
    "    test_loss /= total\n",
    "\n",
    "    return accuracy, precision, recall, f1, fpr, tpr, test_loss\n",
    "\n",
    "# -------------------\n",
    "# Local Training Function (with Loss/Accuracy Logging)\n",
    "# -------------------\n",
    "def local_train(model, train_loader, epochs, device):\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    for epoch in range(epochs):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        total_loss = 0\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * data.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += data.size(0)\n",
    "        avg_loss = total_loss / total\n",
    "        avg_acc = correct / total\n",
    "        print(f\"Local Epoch {epoch+1} - Loss: {avg_loss:.4f}, Accuracy: {avg_acc:.4f}\")\n",
    "    return model.state_dict()\n",
    "\n",
    "# -------------------\n",
    "# Additional Metric Functions\n",
    "# -------------------\n",
    "def compute_model_drift(prev_weights, new_weights):\n",
    "    return sum(torch.norm(prev_weights[k] - new_weights[k]) for k in prev_weights) / len(prev_weights)\n",
    "\n",
    "def compute_entropy(weights):\n",
    "    weights = np.array(weights)\n",
    "    return -np.sum(weights * np.log(weights + 1e-10))\n",
    "\n",
    "def detect_malicious_clients(true_ids, detected_ids):\n",
    "    true_set = set(true_ids)\n",
    "    detected_set = set(detected_ids)\n",
    "    tp = len(true_set & detected_set)\n",
    "    fn = len(true_set - detected_set)\n",
    "    fp = len(detected_set - true_set)\n",
    "    attack_detection_rate = tp / (tp + fn + 1e-6)\n",
    "    exclusion_rate = len(detected_set) / (len(true_set | detected_set) + 1e-6)\n",
    "    return attack_detection_rate, exclusion_rate\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "prev_global_model = {}\n",
    "# -------------------\n",
    "# Aggregation Functions (Full Implementations)\n",
    "# -------------------\n",
    "\n",
    "global detected_malicious_ids\n",
    "detected_malicious_ids = []\n",
    "global aggregation_weights\n",
    "aggregation_weights = []\n",
    "\n",
    "def fltrust(updates):\n",
    "    reference_model = updates[0]  # trusted server model, in real FLTrust, it should be fixed\n",
    "    scores = []\n",
    "    for update in updates:\n",
    "        cosine_sim = sum(torch.nn.functional.cosine_similarity(update[k].flatten(), reference_model[k].flatten(), dim=0) for k in update)\n",
    "        scores.append(cosine_sim / len(update))\n",
    "    weights = torch.softmax(torch.tensor(scores), dim=0)\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = [i for i, w in enumerate(weights) if w < 0.01]\n",
    "    aggregation_weights = weights.tolist()\n",
    "    return {k: sum(weights[i] * updates[i][k] for i in range(len(updates))) for k in updates[0]}\n",
    "\n",
    "def flame(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    norms = [sum(torch.norm(u[k]) for k in u) for u in updates]\n",
    "    weights = torch.softmax(-torch.tensor(norms), dim=0)\n",
    "    aggregation_weights = weights.tolist()\n",
    "    norms = [sum(torch.norm(u[k]) for k in u) for u in updates]\n",
    "    weights = torch.softmax(-torch.tensor(norms), dim=0)\n",
    "    return {k: sum(weights[i] * updates[i][k] for i in range(len(updates))) for k in updates[0]}\n",
    "\n",
    "def flcert(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    aggregation_weights = []\n",
    "\n",
    "    # Step 1: Compute the mean update (global reference)\n",
    "    avg = fedavg(updates)\n",
    "\n",
    "    # Step 2: Compute cosine similarity between each update and the global mean\n",
    "    cosine_similarities = []\n",
    "    for u in updates:\n",
    "        sim = torch.stack([F.cosine_similarity(u[k].flatten(), avg[k].flatten(), dim=0) for k in u]).mean()\n",
    "        cosine_similarities.append(sim.item())\n",
    "\n",
    "    # Step 3: Thresholding (e.g., keep clients above 0.5 similarity)\n",
    "    threshold = 0.5\n",
    "    filtered_indices = [i for i, sim in enumerate(cosine_similarities) if sim >= threshold]\n",
    "    detected_malicious_ids = [i for i in range(len(updates)) if i not in filtered_indices]\n",
    "\n",
    "    if filtered_indices:\n",
    "        filtered_updates = [updates[i] for i in filtered_indices]\n",
    "        aggregation_weights = [1.0 / len(filtered_indices) if i in filtered_indices else 0.0 for i in range(len(updates))]\n",
    "        return fedavg(filtered_updates)\n",
    "    else:\n",
    "        aggregation_weights = [0.0] * len(updates)\n",
    "        return avg\n",
    "    avg = fedavg(updates)\n",
    "    filtered = [u for u in updates if sum(torch.norm(u[k] - avg[k]) for k in u) < 5.0]\n",
    "    return fedavg(filtered) if filtered else avg\n",
    "\n",
    "def foolsgold(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    gradients = [torch.cat([p.flatten() for p in u.values()]) for u in updates]\n",
    "    n = len(gradients)\n",
    "    sim = torch.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                sim[i][j] = torch.nn.functional.cosine_similarity(gradients[i], gradients[j], dim=0)\n",
    "    max_sim = sim.max(dim=1).values\n",
    "    weights = 1.0 - max_sim\n",
    "    weights = weights / weights.sum()\n",
    "    aggregation_weights = weights.tolist()\n",
    "    return {k: sum(weights[i] * updates[i][k] for i in range(n)) for k in updates[0]}\n",
    "\n",
    "def rfa(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "\n",
    "    # Step 1: Flatten updates for geometric median computation\n",
    "    vectors = [torch.cat([u[k].flatten() for k in sorted(u.keys())]) for u in updates]\n",
    "\n",
    "    # Step 2: Initialize with coordinate-wise median\n",
    "    stacked = torch.stack(vectors)\n",
    "    median = torch.median(stacked, dim=0)[0]\n",
    "\n",
    "    # Step 3: Iterative Weiszfeld algorithm\n",
    "    for _ in range(5):\n",
    "        distances = torch.stack([torch.norm(v - median) for v in vectors]) + 1e-10\n",
    "        weights = 1.0 / distances\n",
    "        weights /= weights.sum()\n",
    "        median = sum(weights[i] * vectors[i] for i in range(len(vectors)))\n",
    "\n",
    "    # Step 4: Unflatten median back to model format\n",
    "    example = updates[0]\n",
    "    aggregated = {}\n",
    "    pointer = 0\n",
    "    for k in sorted(example.keys()):\n",
    "        shape = example[k].shape\n",
    "        numel = example[k].numel()\n",
    "        aggregated[k] = median[pointer:pointer + numel].view(shape)\n",
    "        pointer += numel\n",
    "\n",
    "    # Mark clients with high distance as malicious\n",
    "    detected_malicious_ids = [i for i in range(len(vectors)) if torch.norm(vectors[i] - median) > 2.0]\n",
    "    aggregation_weights = [1.0 / len(vectors)] * len(vectors)\n",
    "\n",
    "    return aggregated\n",
    "    mean_update = fedavg(updates)\n",
    "    filtered = [u for u in updates if sum(torch.norm(u[k] - mean_update[k]) for k in u) < 3.0]\n",
    "    return fedavg(filtered) if filtered else mean_update\n",
    "\n",
    "def auror(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "\n",
    "    # Step 1: Flatten all updates\n",
    "    flat_updates = [torch.cat([u[k].flatten() for k in sorted(u.keys())]) for u in updates]\n",
    "\n",
    "    # Step 2: Compute pairwise L2 distances\n",
    "    num_clients = len(flat_updates)\n",
    "    distances = torch.zeros((num_clients, num_clients))\n",
    "    for i in range(num_clients):\n",
    "        for j in range(i + 1, num_clients):\n",
    "            dist = torch.norm(flat_updates[i] - flat_updates[j]).item()\n",
    "            distances[i][j] = distances[j][i] = dist\n",
    "\n",
    "    # Step 3: Compute score based on distance to k nearest neighbors\n",
    "    k = max(1, num_clients // 10)\n",
    "    scores = []\n",
    "    for i in range(num_clients):\n",
    "        sorted_dists = torch.sort(distances[i])[0]\n",
    "        scores.append(sorted_dists[1:k + 1].mean().item())  # Exclude self-distance\n",
    "\n",
    "    # Step 4: Select clients with lowest scores (most consistent)\n",
    "    sorted_ids = sorted(range(num_clients), key=lambda i: scores[i])\n",
    "    selected_ids = sorted_ids[:num_clients // 2]\n",
    "\n",
    "    detected_malicious_ids = [i for i in range(num_clients) if i not in selected_ids]\n",
    "    aggregation_weights = [1.0 / len(selected_ids) if i in selected_ids else 0.0 for i in range(num_clients)]\n",
    "\n",
    "    return fedavg([updates[i] for i in selected_ids])\n",
    "    #global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    scores = []\n",
    "    for i in range(len(updates)):\n",
    "        sim_sum = sum(torch.sum((updates[i][k] - updates[j][k])**2) for j in range(len(updates)) if j != i for k in updates[i])\n",
    "        scores.append(sim_sum)\n",
    "    best_ids = sorted(range(len(scores)), key=lambda i: scores[i])[:len(updates)//2]\n",
    "    return fedavg([updates[i] for i in best_ids])\n",
    "\n",
    "def bulyan(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    aggregation_weights = []\n",
    "\n",
    "    # Step 1: Compute distances between updates\n",
    "    n = len(updates)\n",
    "    distances = torch.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            d = sum(torch.norm(updates[i][k] - updates[j][k])**2 for k in updates[i])\n",
    "            distances[i][j] = distances[j][i] = d\n",
    "\n",
    "    # Step 2: Krum candidate selection (select n - 2f - 2 clients)\n",
    "    f = max(1, n // 10)\n",
    "    krum_candidates = []\n",
    "    for i in range(n):\n",
    "        sorted_dists = sorted(distances[i][j] for j in range(n) if j != i)\n",
    "        krum_score = sum(sorted_dists[:n - f - 2])\n",
    "        krum_candidates.append((i, krum_score))\n",
    "    krum_candidates.sort(key=lambda x: x[1])\n",
    "    selected_ids = [idx for idx, _ in krum_candidates[:n - 2 * f]]\n",
    "\n",
    "    # Step 3: For each parameter, collect values and perform trimmed mean\n",
    "    trimmed_updates = [updates[i] for i in selected_ids]\n",
    "    aggregation_weights = [1.0 / len(trimmed_updates) if i in selected_ids else 0.0 for i in range(n)]\n",
    "    detected_malicious_ids = [i for i in range(n) if i not in selected_ids]\n",
    "\n",
    "    return trimmed_mean(trimmed_updates)\n",
    "\n",
    "def rfed(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "\n",
    "    # Flatten each update and compute local variances\n",
    "    flat_updates = [torch.cat([u[k].flatten() for k in sorted(u.keys())]) for u in updates]\n",
    "    stacked = torch.stack(flat_updates)\n",
    "    feature_variance = torch.var(stacked, dim=0)\n",
    "\n",
    "    # Compute Mahalanobis-like distance from global variance\n",
    "    distances = []\n",
    "    for i in range(len(flat_updates)):\n",
    "        diff = flat_updates[i] - stacked.mean(dim=0)\n",
    "        dist = (diff ** 2 / (feature_variance + 1e-6)).mean()\n",
    "        distances.append(dist)\n",
    "\n",
    "    weights = torch.softmax(-torch.tensor(distances), dim=0)\n",
    "    aggregation_weights = weights.tolist()\n",
    "\n",
    "    # Aggregate\n",
    "    aggregated = {}\n",
    "    for k in updates[0].keys():\n",
    "        aggregated[k] = sum(weights[i] * updates[i][k] for i in range(len(updates)))\n",
    "\n",
    "    return aggregated\n",
    "\n",
    "def fedavg(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    aggregation_weights = [1.0 / len(updates)] * len(updates)\n",
    "    return {k: torch.stack([u[k] for u in updates], 0).mean(0) for k in updates[0].keys()}\n",
    "\n",
    "def median(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    aggregation_weights = [1.0 / len(updates)] * len(updates)\n",
    "    return {k: torch.median(torch.stack([u[k] for u in updates], 0), dim=0)[0] for k in updates[0].keys()}\n",
    "\n",
    "def trimmed_mean(updates, beta=0.1):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    aggregation_weights = [1.0 / len(updates)] * len(updates)\n",
    "    n = len(updates)\n",
    "    trim = int(n * beta)\n",
    "    agg = {}\n",
    "    for k in updates[0].keys():\n",
    "        stacked = torch.stack([u[k] for u in updates], dim=0)\n",
    "        sorted_vals, _ = stacked.sort(dim=0)\n",
    "        trimmed = sorted_vals[trim:n-trim]\n",
    "        agg[k] = trimmed.mean(dim=0)\n",
    "    return agg\n",
    "\n",
    "def krum(updates, f=1):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    aggregation_weights = [1.0 / len(updates)] * len(updates)\n",
    "    scores = []\n",
    "    for i, ui in enumerate(updates):\n",
    "        dists = []\n",
    "        for j, uj in enumerate(updates):\n",
    "            if i != j:\n",
    "                dist = sum(torch.norm(ui[k] - uj[k])**2 for k in ui)\n",
    "                dists.append(dist)\n",
    "        dists.sort()\n",
    "        scores.append((i, sum(dists[:len(updates)-f-2])) if len(dists) > f+2 else (i, float('inf')))\n",
    "    selected = min(scores, key=lambda x: x[1])[0]\n",
    "    return updates[selected]\n",
    "\n",
    "AGGREGATION_FUNCTIONS = {\n",
    "    'FedAVG': fedavg,\n",
    "    'TrimmedMean': trimmed_mean,\n",
    "    'Median': median,\n",
    "    'Krum': krum,\n",
    "    'FLTrust': fltrust,\n",
    "    'FLAME': flame,\n",
    "    'FLCert': flcert,\n",
    "    'FoolsGold': foolsgold,\n",
    "    'RFA': rfa,\n",
    "    'Auror': auror,\n",
    "    'Bulyan': bulyan,\n",
    "    'RFed': rfed\n",
    "}\n",
    "\n",
    "\n",
    "# -------------------\n",
    "# Federated Training Function\n",
    "# -------------------\n",
    "def federated_training(rounds, epochs, client_fraction, attack_types, attack_ratios, defense_method, iid, alpha, device):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    client_data = partition_data(mnist_train, num_clients=100, alpha=alpha, iid=iid)\n",
    "\n",
    "    global malicious_client_ids\n",
    "    malicious_client_ids = []\n",
    "    if attack_types and attack_ratios:\n",
    "        client_data = apply_combined_attacks(client_data, attack_types, attack_ratios)\n",
    "\n",
    "    global_model = SimpleCNN().to(device)\n",
    "    global_weights = global_model.state_dict()\n",
    "    prev_weights = {k: v.clone() for k, v in global_weights.items()}\n",
    "\n",
    "    results = []\n",
    "    test_loader = DataLoader(mnist_test, batch_size=128, shuffle=False)\n",
    "\n",
    "    for rnd in range(rounds):\n",
    "        global detected_malicious_ids, aggregation_weights\n",
    "        detected_malicious_ids = []\n",
    "        aggregation_weights = []\n",
    "\n",
    "        selected_clients = random.sample(list(client_data.keys()), int(client_fraction * len(client_data)))\n",
    "        local_updates = []\n",
    "        total_train_loss = 0.0\n",
    "        total_train_correct = 0\n",
    "        total_train_samples = 0\n",
    "\n",
    "        for cid in selected_clients:\n",
    "            local_model = SimpleCNN().to(device)\n",
    "            local_model.load_state_dict(global_weights)\n",
    "            optimizer = optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "            local_model.train()\n",
    "            for _ in range(epochs):\n",
    "                for x, y in client_data[cid]:\n",
    "                    x, y = x.to(device), y.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    output = local_model(x)\n",
    "                    loss = F.cross_entropy(output, y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    total_train_loss += loss.item() * x.size(0)\n",
    "                    total_train_correct += (output.argmax(1) == y).sum().item()\n",
    "                    total_train_samples += x.size(0)\n",
    "\n",
    "            local_updates.append({k: v.cpu().detach() for k, v in local_model.state_dict().items()})\n",
    "\n",
    "        if defense_method in AGGREGATION_FUNCTIONS:\n",
    "            global_weights = AGGREGATION_FUNCTIONS[defense_method](local_updates)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown defense method: {defense_method}\")\n",
    "\n",
    "        global_model.load_state_dict(global_weights)\n",
    "\n",
    "        acc, prec, rec, f1, fpr, tpr, test_loss = evaluate_model(global_model, test_loader, device)\n",
    "        attack_acc, exclusion_rate = detect_malicious_clients(malicious_client_ids, detected_malicious_ids)\n",
    "        entropy = compute_entropy(aggregation_weights) if aggregation_weights else 0.0\n",
    "        drift = compute_model_drift(prev_weights, global_weights)\n",
    "        prev_weights = {k: v.clone() for k, v in global_weights.items()}\n",
    "\n",
    "        avg_train_loss = total_train_loss / total_train_samples if total_train_samples > 0 else 0.0\n",
    "        avg_train_acc = total_train_correct / total_train_samples if total_train_samples > 0 else 0.0\n",
    "\n",
    "        results.append({\n",
    "            'Round': rnd + 1, 'Accuracy': acc, 'Precision': prec, 'Recall': rec, 'F1': f1,\n",
    "            'FPR': fpr, 'TPR': tpr, 'TestLoss': test_loss,\n",
    "            'TrainLoss': avg_train_loss, 'TrainAccuracy': avg_train_acc,\n",
    "            'AttackDetectAcc': attack_acc, 'ExclusionRate': exclusion_rate,\n",
    "            'Entropy': entropy, 'ModelDrift': drift\n",
    "        })\n",
    "\n",
    "        print(f\"Round {rnd + 1}: Acc={acc:.4f}, Prec={prec:.4f}, Rec={rec:.4f}, F1={f1:.4f}, \"\n",
    "              f\"FPR={fpr:.4f}, TPR={tpr:.4f}, TestLoss={test_loss:.4f}, TrainLoss={avg_train_loss:.4f}, \"\n",
    "              f\"TrainAcc={avg_train_acc:.4f}, AttackDetectAcc={attack_acc:.4f}, \"\n",
    "              f\"ExclusionRate={exclusion_rate:.4f}, Entropy={entropy:.4f}, Drift={drift:.4f}\")\n",
    "\n",
    "    pd.DataFrame(results).to_csv('fl_full_results.csv', index=False)\n",
    "    print(\"Federated training completed. Metrics saved to fl_full_results.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee665649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1: Acc=0.0984, Prec=0.0498, Rec=0.1002, F1=0.0183, FPR=0.1000, TPR=0.1002, TestLoss=2.2955, TrainLoss=1.7670, TrainAcc=0.4308, AttackDetectAcc=0.0000, ExclusionRate=0.0000, Entropy=2.3026, Drift=0.0559\n",
      "Round 2: Acc=0.2314, Prec=0.1705, Rec=0.2333, F1=0.1168, FPR=0.0854, TPR=0.2333, TestLoss=2.2576, TrainLoss=1.7336, TrainAcc=0.4759, AttackDetectAcc=0.0000, ExclusionRate=0.0000, Entropy=2.3026, Drift=0.0527\n",
      "Round 3: Acc=0.1032, Prec=0.0103, Rec=0.1000, F1=0.0187, FPR=0.1000, TPR=0.1000, TestLoss=2.2593, TrainLoss=1.4211, TrainAcc=0.5157, AttackDetectAcc=0.0000, ExclusionRate=0.0000, Entropy=2.3026, Drift=0.0542\n",
      "Round 4: Acc=0.2715, Prec=0.2069, Rec=0.2718, F1=0.1652, FPR=0.0810, TPR=0.2718, TestLoss=2.1499, TrainLoss=1.4445, TrainAcc=0.5185, AttackDetectAcc=0.0000, ExclusionRate=0.0000, Entropy=2.3026, Drift=0.0445\n",
      "Round 5: Acc=0.2411, Prec=0.4312, Rec=0.2430, F1=0.2003, FPR=0.0841, TPR=0.2430, TestLoss=1.9956, TrainLoss=1.3034, TrainAcc=0.5988, AttackDetectAcc=0.0000, ExclusionRate=0.0000, Entropy=2.3026, Drift=0.0609\n",
      "Federated training completed. Metrics saved to fl_full_results.csv\n"
     ]
    }
   ],
   "source": [
    "federated_training(\n",
    "    rounds=5,\n",
    "    epochs=2,\n",
    "    client_fraction=0.1,\n",
    "    attack_types=['label_flipping', 'feature_manipulation'],\n",
    "    attack_ratios={'label_flipping': 0.2, 'feature_manipulation': 0.3},\n",
    "    defense_method='Median',\n",
    "    iid=False,\n",
    "    alpha=0.3,\n",
    "    device='cpu'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbf62c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1: Acc=0.0980, Prec=0.0098, Rec=0.1000, F1=0.0179, FPR=0.1000, TPR=0.1000, TestLoss=2.3038, TrainLoss=1.7923, TrainAcc=0.4145, AttackDetectAcc=0.0000, ExclusionRate=0.0000, Entropy=2.3026, Drift=0.0623\n",
      "Round 2: Acc=0.0980, Prec=0.0098, Rec=0.1000, F1=0.0179, FPR=0.1000, TPR=0.1000, TestLoss=2.2894, TrainLoss=1.6175, TrainAcc=0.4624, AttackDetectAcc=0.0000, ExclusionRate=0.0000, Entropy=2.3026, Drift=0.0373\n",
      "Round 3: Acc=0.1935, Prec=0.0540, Rec=0.1945, F1=0.0804, FPR=0.0897, TPR=0.1945, TestLoss=2.2836, TrainLoss=1.3730, TrainAcc=0.5680, AttackDetectAcc=0.0000, ExclusionRate=0.0000, Entropy=2.3026, Drift=0.0644\n",
      "Round 4: Acc=0.1949, Prec=0.2031, Rec=0.1950, F1=0.1193, FPR=0.0895, TPR=0.1950, TestLoss=2.1320, TrainLoss=1.3247, TrainAcc=0.5618, AttackDetectAcc=0.0000, ExclusionRate=0.0000, Entropy=2.3026, Drift=0.0500\n",
      "Round 5: Acc=0.5412, Prec=0.6531, Rec=0.5310, F1=0.4821, FPR=0.0510, TPR=0.5310, TestLoss=1.8198, TrainLoss=1.0280, TrainAcc=0.6806, AttackDetectAcc=0.0000, ExclusionRate=0.0000, Entropy=2.3026, Drift=0.0722\n",
      "Federated training completed. Metrics saved to fl_full_results.csv\n",
      "\n",
      "Total Training Time: 1 minutes and 13 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Execute federated training\n",
    "federated_training(\n",
    "    rounds=5,\n",
    "    epochs=2,\n",
    "    client_fraction=0.1,\n",
    "    attack_types=['label_flipping', 'feature_manipulation'],\n",
    "    attack_ratios={'label_flipping': 0.2, 'feature_manipulation': 0.3},\n",
    "    defense_method='Median',\n",
    "    iid=False,\n",
    "    alpha=0.3,\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Compute elapsed time in minutes and seconds\n",
    "elapsed = end_time - start_time\n",
    "minutes = int(elapsed // 60)\n",
    "seconds = int(elapsed % 60)\n",
    "\n",
    "print(f\"\\nTotal Training Time: {minutes} minutes and {seconds} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d376f5eb",
   "metadata": {},
   "source": [
    "### Targeted Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39cee9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# Full Implementation with Attack Selection, Percentage Control, and Real Defense Methods\n",
    "# -------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "import math\n",
    "\n",
    "# -------------------\n",
    "# CNN Model\n",
    "# -------------------\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# -------------------\n",
    "# Data Partition (IID or non-IID using Dirichlet)\n",
    "# -------------------\n",
    "def partition_data(dataset, num_clients=100, alpha=0.5, iid=True):\n",
    "    if iid:\n",
    "        data_split = torch.utils.data.random_split(dataset, [len(dataset) // num_clients] * num_clients)\n",
    "    else:\n",
    "        labels = np.array(dataset.targets)\n",
    "        idx_by_class = [np.where(labels == i)[0] for i in range(10)]\n",
    "        data_split = [[] for _ in range(num_clients)]\n",
    "        for c, idx in enumerate(idx_by_class):\n",
    "            np.random.shuffle(idx)\n",
    "            proportions = np.random.dirichlet(np.repeat(alpha, num_clients))\n",
    "            proportions = (np.cumsum(proportions) * len(idx)).astype(int)[:-1]\n",
    "            splits = np.split(idx, proportions)\n",
    "            for client_id, split in enumerate(splits):\n",
    "                data_split[client_id].extend(split)\n",
    "        data_split = [Subset(dataset, idxs) for idxs in data_split]\n",
    "    return {i: DataLoader(data_split[i], batch_size=32, shuffle=True) for i in range(num_clients)}\n",
    "\n",
    "# -------------------------\n",
    "# Global Attack Configuration\n",
    "# -------------------------\n",
    "TARGET_LABEL_FOR_POISONING = 0\n",
    "TARGET_LABEL_FLIP_MAP = {1: 7, 3: 8}  # Only flip 1→7, 3→8\n",
    "FEATURE_MANIPULATION_TARGET_CLASS = 8\n",
    "malicious_client_ids = []\n",
    "\n",
    "# -------------------------\n",
    "# Targeted Label Flipping Attack\n",
    "# -------------------------\n",
    "def apply_targeted_label_flipping_attack(loader):\n",
    "    \"\"\"\n",
    "    Flip labels of specific classes (1 → 7, 3 → 8) in the MNIST dataset.\n",
    "    Other labels are left unchanged.\n",
    "    \"\"\"\n",
    "    attacked = []\n",
    "    for x, y in loader:\n",
    "        y_flipped = torch.tensor([\n",
    "            TARGET_LABEL_FLIP_MAP.get(int(label), int(label)) for label in y\n",
    "        ])\n",
    "        attacked.extend([(x[i], y_flipped[i]) for i in range(len(x))])\n",
    "    x_tensor = torch.stack([d[0] for d in attacked])\n",
    "    y_tensor = torch.tensor([d[1] for d in attacked])\n",
    "    return DataLoader(TensorDataset(x_tensor, y_tensor), batch_size=32, shuffle=True)\n",
    "\n",
    "# -------------------------\n",
    "# Targeted Feature Manipulation Attack\n",
    "# -------------------------\n",
    "def apply_targeted_feature_manipulation_attack(loader):\n",
    "    \"\"\"\n",
    "    Adds structured noise only to images of label 8.\n",
    "    Other inputs remain unchanged.\n",
    "    \"\"\"\n",
    "    attacked = []\n",
    "    for x, y in loader:\n",
    "        x_attacked = x.clone()\n",
    "        for i in range(len(x)):\n",
    "            if y[i] == FEATURE_MANIPULATION_TARGET_CLASS:\n",
    "                noise = torch.randn_like(x[i]) * 0.5\n",
    "                mask = (torch.rand_like(x[i]) > 0.6).float()\n",
    "                x_attacked[i] = x[i] * mask + noise * (1 - mask)\n",
    "        attacked.extend([(x_attacked[i], y[i]) for i in range(len(x))])\n",
    "    x_tensor = torch.stack([d[0] for d in attacked])\n",
    "    y_tensor = torch.tensor([d[1] for d in attacked])\n",
    "    return DataLoader(TensorDataset(x_tensor, y_tensor), batch_size=32, shuffle=True)\n",
    "\n",
    "# -------------------------\n",
    "# Targeted Data Poisoning Attack (Backdoor)\n",
    "# -------------------------\n",
    "def apply_targeted_poisoning_attack(loader):\n",
    "    \"\"\"\n",
    "    Adds a white square trigger to each image and changes its label to the target (0).\n",
    "    \"\"\"\n",
    "    attacked = []\n",
    "    for x, y in loader:\n",
    "        x_poisoned = torch.clamp(x + 0.2, 0, 1)  # Global intensity shift\n",
    "        trigger = torch.zeros_like(x_poisoned)\n",
    "        trigger[:, :, -3:, -3:] = 1.0  # Add white square at bottom-right corner\n",
    "        x_poisoned = torch.clamp(x_poisoned + trigger, 0, 1)\n",
    "        y_target = torch.full_like(y, TARGET_LABEL_FOR_POISONING)\n",
    "        attacked.extend([(x_poisoned[i], y_target[i]) for i in range(len(x))])\n",
    "    x_tensor = torch.stack([d[0] for d in attacked])\n",
    "    y_tensor = torch.tensor([d[1] for d in attacked])\n",
    "    return DataLoader(TensorDataset(x_tensor, y_tensor), batch_size=32, shuffle=True)\n",
    "\n",
    "# -------------------------\n",
    "# Combined Targeted Attack Function\n",
    "# -------------------------\n",
    "def apply_combined_targeted_attacks(client_data, attack_types, attack_ratios):\n",
    "    \"\"\"\n",
    "    Applies selected targeted attacks to a subset of clients based on the given ratios.\n",
    "    :param client_data: dictionary of {client_id: DataLoader}\n",
    "    :param attack_types: list of attacks to apply: ['label_flipping', 'feature_manipulation', 'poisoning']\n",
    "    :param attack_ratios: dictionary of ratios per attack type\n",
    "    :return: modified client_data with attacked data for selected clients\n",
    "    \"\"\"\n",
    "    attack_specific_ids = {}\n",
    "    total_clients = list(client_data.keys())\n",
    "    num_clients = len(total_clients)\n",
    "\n",
    "    # Assign clients to attacks\n",
    "    for attack in attack_types:\n",
    "        count = int(num_clients * attack_ratios.get(attack, 0))\n",
    "        attack_specific_ids[attack] = random.sample(total_clients, count)\n",
    "\n",
    "    # Merge all malicious client IDs\n",
    "    global malicious_client_ids\n",
    "    malicious_client_ids = list(set().union(*attack_specific_ids.values()))\n",
    "\n",
    "    # Apply attack on selected clients\n",
    "    for client_id in malicious_client_ids:\n",
    "        loader = client_data[client_id]\n",
    "        if 'label_flipping' in attack_types and client_id in attack_specific_ids.get('label_flipping', []):\n",
    "            loader = apply_targeted_label_flipping_attack(loader)\n",
    "        if 'feature_manipulation' in attack_types and client_id in attack_specific_ids.get('feature_manipulation', []):\n",
    "            loader = apply_targeted_feature_manipulation_attack(loader)\n",
    "        if 'poisoning' in attack_types and client_id in attack_specific_ids.get('poisoning', []):\n",
    "            loader = apply_targeted_poisoning_attack(loader)\n",
    "        client_data[client_id] = loader\n",
    "\n",
    "    return client_data\n",
    "\n",
    "\n",
    "# -------------------\n",
    "# Evaluation Function\n",
    "# -------------------\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            if isinstance(batch, (list, tuple)) and len(batch) == 2:\n",
    "                data, target = batch\n",
    "            else:\n",
    "                raise ValueError(\"Each batch in test_loader must be a (data, target) tuple.\")\n",
    "\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            total += target.size(0)\n",
    "            y_true.extend(target.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(10)))\n",
    "    tp = np.diag(cm)\n",
    "    fp = np.sum(cm, axis=0) - tp\n",
    "    fn = np.sum(cm, axis=1) - tp\n",
    "    tn = np.sum(cm) - (tp + fp + fn)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    fpr = np.mean(fp / (fp + tn + 1e-6))\n",
    "    tpr = np.mean(tp / (tp + fn + 1e-6))\n",
    "    test_loss /= total\n",
    "\n",
    "    return accuracy, precision, recall, f1, fpr, tpr, test_loss\n",
    "\n",
    "# -------------------\n",
    "# Local Training Function (with Loss/Accuracy Logging)\n",
    "# -------------------\n",
    "def local_train(model, train_loader, epochs, device):\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    for epoch in range(epochs):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        total_loss = 0\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * data.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += data.size(0)\n",
    "        avg_loss = total_loss / total\n",
    "        avg_acc = correct / total\n",
    "        print(f\"Local Epoch {epoch+1} - Loss: {avg_loss:.4f}, Accuracy: {avg_acc:.4f}\")\n",
    "    return model.state_dict()\n",
    "\n",
    "# -------------------\n",
    "# Additional Metric Functions\n",
    "# -------------------\n",
    "def compute_model_drift(prev_weights, new_weights):\n",
    "    return sum(torch.norm(prev_weights[k] - new_weights[k]) for k in prev_weights) / len(prev_weights)\n",
    "\n",
    "def compute_entropy(weights):\n",
    "    weights = np.array(weights)\n",
    "    return -np.sum(weights * np.log(weights + 1e-10))\n",
    "\n",
    "def detect_malicious_clients(true_ids, detected_ids):\n",
    "    true_set = set(true_ids)\n",
    "    detected_set = set(detected_ids)\n",
    "    tp = len(true_set & detected_set)\n",
    "    fn = len(true_set - detected_set)\n",
    "    fp = len(detected_set - true_set)\n",
    "    attack_detection_rate = tp / (tp + fn + 1e-6)\n",
    "    exclusion_rate = len(detected_set) / (len(true_set | detected_set) + 1e-6)\n",
    "    return attack_detection_rate, exclusion_rate\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "prev_global_model = {}\n",
    "# -------------------\n",
    "# Aggregation Functions (Full Implementations)\n",
    "# -------------------\n",
    "\n",
    "global detected_malicious_ids\n",
    "detected_malicious_ids = []\n",
    "global aggregation_weights\n",
    "aggregation_weights = []\n",
    "\n",
    "def fltrust(updates):\n",
    "    reference_model = updates[0]  # trusted server model, in real FLTrust, it should be fixed\n",
    "    scores = []\n",
    "    for update in updates:\n",
    "        cosine_sim = sum(torch.nn.functional.cosine_similarity(update[k].flatten(), reference_model[k].flatten(), dim=0) for k in update)\n",
    "        scores.append(cosine_sim / len(update))\n",
    "    weights = torch.softmax(torch.tensor(scores), dim=0)\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = [i for i, w in enumerate(weights) if w < 0.01]\n",
    "    aggregation_weights = weights.tolist()\n",
    "    return {k: sum(weights[i] * updates[i][k] for i in range(len(updates))) for k in updates[0]}\n",
    "\n",
    "def flame(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    norms = [sum(torch.norm(u[k]) for k in u) for u in updates]\n",
    "    weights = torch.softmax(-torch.tensor(norms), dim=0)\n",
    "    aggregation_weights = weights.tolist()\n",
    "    norms = [sum(torch.norm(u[k]) for k in u) for u in updates]\n",
    "    weights = torch.softmax(-torch.tensor(norms), dim=0)\n",
    "    return {k: sum(weights[i] * updates[i][k] for i in range(len(updates))) for k in updates[0]}\n",
    "\n",
    "def flcert(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    aggregation_weights = []\n",
    "\n",
    "    # Step 1: Compute the mean update (global reference)\n",
    "    avg = fedavg(updates)\n",
    "\n",
    "    # Step 2: Compute cosine similarity between each update and the global mean\n",
    "    cosine_similarities = []\n",
    "    for u in updates:\n",
    "        sim = torch.stack([F.cosine_similarity(u[k].flatten(), avg[k].flatten(), dim=0) for k in u]).mean()\n",
    "        cosine_similarities.append(sim.item())\n",
    "\n",
    "    # Step 3: Thresholding (e.g., keep clients above 0.5 similarity)\n",
    "    threshold = 0.5\n",
    "    filtered_indices = [i for i, sim in enumerate(cosine_similarities) if sim >= threshold]\n",
    "    detected_malicious_ids = [i for i in range(len(updates)) if i not in filtered_indices]\n",
    "\n",
    "    if filtered_indices:\n",
    "        filtered_updates = [updates[i] for i in filtered_indices]\n",
    "        aggregation_weights = [1.0 / len(filtered_indices) if i in filtered_indices else 0.0 for i in range(len(updates))]\n",
    "        return fedavg(filtered_updates)\n",
    "    else:\n",
    "        aggregation_weights = [0.0] * len(updates)\n",
    "        return avg\n",
    "    avg = fedavg(updates)\n",
    "    filtered = [u for u in updates if sum(torch.norm(u[k] - avg[k]) for k in u) < 5.0]\n",
    "    return fedavg(filtered) if filtered else avg\n",
    "\n",
    "def foolsgold(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    gradients = [torch.cat([p.flatten() for p in u.values()]) for u in updates]\n",
    "    n = len(gradients)\n",
    "    sim = torch.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                sim[i][j] = torch.nn.functional.cosine_similarity(gradients[i], gradients[j], dim=0)\n",
    "    max_sim = sim.max(dim=1).values\n",
    "    weights = 1.0 - max_sim\n",
    "    weights = weights / weights.sum()\n",
    "    aggregation_weights = weights.tolist()\n",
    "    return {k: sum(weights[i] * updates[i][k] for i in range(n)) for k in updates[0]}\n",
    "\n",
    "def rfa(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "\n",
    "    # Step 1: Flatten updates for geometric median computation\n",
    "    vectors = [torch.cat([u[k].flatten() for k in sorted(u.keys())]) for u in updates]\n",
    "\n",
    "    # Step 2: Initialize with coordinate-wise median\n",
    "    stacked = torch.stack(vectors)\n",
    "    median = torch.median(stacked, dim=0)[0]\n",
    "\n",
    "    # Step 3: Iterative Weiszfeld algorithm\n",
    "    for _ in range(5):\n",
    "        distances = torch.stack([torch.norm(v - median) for v in vectors]) + 1e-10\n",
    "        weights = 1.0 / distances\n",
    "        weights /= weights.sum()\n",
    "        median = sum(weights[i] * vectors[i] for i in range(len(vectors)))\n",
    "\n",
    "    # Step 4: Unflatten median back to model format\n",
    "    example = updates[0]\n",
    "    aggregated = {}\n",
    "    pointer = 0\n",
    "    for k in sorted(example.keys()):\n",
    "        shape = example[k].shape\n",
    "        numel = example[k].numel()\n",
    "        aggregated[k] = median[pointer:pointer + numel].view(shape)\n",
    "        pointer += numel\n",
    "\n",
    "    # Mark clients with high distance as malicious\n",
    "    detected_malicious_ids = [i for i in range(len(vectors)) if torch.norm(vectors[i] - median) > 2.0]\n",
    "    aggregation_weights = [1.0 / len(vectors)] * len(vectors)\n",
    "\n",
    "    return aggregated\n",
    "    mean_update = fedavg(updates)\n",
    "    filtered = [u for u in updates if sum(torch.norm(u[k] - mean_update[k]) for k in u) < 3.0]\n",
    "    return fedavg(filtered) if filtered else mean_update\n",
    "\n",
    "def auror(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "\n",
    "    # Step 1: Flatten all updates\n",
    "    flat_updates = [torch.cat([u[k].flatten() for k in sorted(u.keys())]) for u in updates]\n",
    "\n",
    "    # Step 2: Compute pairwise L2 distances\n",
    "    num_clients = len(flat_updates)\n",
    "    distances = torch.zeros((num_clients, num_clients))\n",
    "    for i in range(num_clients):\n",
    "        for j in range(i + 1, num_clients):\n",
    "            dist = torch.norm(flat_updates[i] - flat_updates[j]).item()\n",
    "            distances[i][j] = distances[j][i] = dist\n",
    "\n",
    "    # Step 3: Compute score based on distance to k nearest neighbors\n",
    "    k = max(1, num_clients // 10)\n",
    "    scores = []\n",
    "    for i in range(num_clients):\n",
    "        sorted_dists = torch.sort(distances[i])[0]\n",
    "        scores.append(sorted_dists[1:k + 1].mean().item())  # Exclude self-distance\n",
    "\n",
    "    # Step 4: Select clients with lowest scores (most consistent)\n",
    "    sorted_ids = sorted(range(num_clients), key=lambda i: scores[i])\n",
    "    selected_ids = sorted_ids[:num_clients // 2]\n",
    "\n",
    "    detected_malicious_ids = [i for i in range(num_clients) if i not in selected_ids]\n",
    "    aggregation_weights = [1.0 / len(selected_ids) if i in selected_ids else 0.0 for i in range(num_clients)]\n",
    "\n",
    "    return fedavg([updates[i] for i in selected_ids])\n",
    "    #global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    scores = []\n",
    "    for i in range(len(updates)):\n",
    "        sim_sum = sum(torch.sum((updates[i][k] - updates[j][k])**2) for j in range(len(updates)) if j != i for k in updates[i])\n",
    "        scores.append(sim_sum)\n",
    "    best_ids = sorted(range(len(scores)), key=lambda i: scores[i])[:len(updates)//2]\n",
    "    return fedavg([updates[i] for i in best_ids])\n",
    "\n",
    "def bulyan(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    aggregation_weights = []\n",
    "\n",
    "    # Step 1: Compute distances between updates\n",
    "    n = len(updates)\n",
    "    distances = torch.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            d = sum(torch.norm(updates[i][k] - updates[j][k])**2 for k in updates[i])\n",
    "            distances[i][j] = distances[j][i] = d\n",
    "\n",
    "    # Step 2: Krum candidate selection (select n - 2f - 2 clients)\n",
    "    f = max(1, n // 10)\n",
    "    krum_candidates = []\n",
    "    for i in range(n):\n",
    "        sorted_dists = sorted(distances[i][j] for j in range(n) if j != i)\n",
    "        krum_score = sum(sorted_dists[:n - f - 2])\n",
    "        krum_candidates.append((i, krum_score))\n",
    "    krum_candidates.sort(key=lambda x: x[1])\n",
    "    selected_ids = [idx for idx, _ in krum_candidates[:n - 2 * f]]\n",
    "\n",
    "    # Step 3: For each parameter, collect values and perform trimmed mean\n",
    "    trimmed_updates = [updates[i] for i in selected_ids]\n",
    "    aggregation_weights = [1.0 / len(trimmed_updates) if i in selected_ids else 0.0 for i in range(n)]\n",
    "    detected_malicious_ids = [i for i in range(n) if i not in selected_ids]\n",
    "\n",
    "    return trimmed_mean(trimmed_updates)\n",
    "\n",
    "def rfed(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "\n",
    "    # Flatten each update and compute local variances\n",
    "    flat_updates = [torch.cat([u[k].flatten() for k in sorted(u.keys())]) for u in updates]\n",
    "    stacked = torch.stack(flat_updates)\n",
    "    feature_variance = torch.var(stacked, dim=0)\n",
    "\n",
    "    # Compute Mahalanobis-like distance from global variance\n",
    "    distances = []\n",
    "    for i in range(len(flat_updates)):\n",
    "        diff = flat_updates[i] - stacked.mean(dim=0)\n",
    "        dist = (diff ** 2 / (feature_variance + 1e-6)).mean()\n",
    "        distances.append(dist)\n",
    "\n",
    "    weights = torch.softmax(-torch.tensor(distances), dim=0)\n",
    "    aggregation_weights = weights.tolist()\n",
    "\n",
    "    # Aggregate\n",
    "    aggregated = {}\n",
    "    for k in updates[0].keys():\n",
    "        aggregated[k] = sum(weights[i] * updates[i][k] for i in range(len(updates)))\n",
    "\n",
    "    return aggregated\n",
    "\n",
    "def fedavg(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    aggregation_weights = [1.0 / len(updates)] * len(updates)\n",
    "    return {k: torch.stack([u[k] for u in updates], 0).mean(0) for k in updates[0].keys()}\n",
    "\n",
    "def median(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    aggregation_weights = [1.0 / len(updates)] * len(updates)\n",
    "    return {k: torch.median(torch.stack([u[k] for u in updates], 0), dim=0)[0] for k in updates[0].keys()}\n",
    "\n",
    "def trimmed_mean(updates, beta=0.1):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    aggregation_weights = [1.0 / len(updates)] * len(updates)\n",
    "    n = len(updates)\n",
    "    trim = int(n * beta)\n",
    "    agg = {}\n",
    "    for k in updates[0].keys():\n",
    "        stacked = torch.stack([u[k] for u in updates], dim=0)\n",
    "        sorted_vals, _ = stacked.sort(dim=0)\n",
    "        trimmed = sorted_vals[trim:n-trim]\n",
    "        agg[k] = trimmed.mean(dim=0)\n",
    "    return agg\n",
    "\n",
    "def krum(updates, f=1):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    aggregation_weights = [1.0 / len(updates)] * len(updates)\n",
    "    scores = []\n",
    "    for i, ui in enumerate(updates):\n",
    "        dists = []\n",
    "        for j, uj in enumerate(updates):\n",
    "            if i != j:\n",
    "                dist = sum(torch.norm(ui[k] - uj[k])**2 for k in ui)\n",
    "                dists.append(dist)\n",
    "        dists.sort()\n",
    "        scores.append((i, sum(dists[:len(updates)-f-2])) if len(dists) > f+2 else (i, float('inf')))\n",
    "    selected = min(scores, key=lambda x: x[1])[0]\n",
    "    return updates[selected]\n",
    "\n",
    "AGGREGATION_FUNCTIONS = {\n",
    "    'FedAVG': fedavg,\n",
    "    'TrimmedMean': trimmed_mean,\n",
    "    'Median': median,\n",
    "    'Krum': krum,\n",
    "    'FLTrust': fltrust,\n",
    "    'FLAME': flame,\n",
    "    'FLCert': flcert,\n",
    "    'FoolsGold': foolsgold,\n",
    "    'RFA': rfa,\n",
    "    'Auror': auror,\n",
    "    'Bulyan': bulyan,\n",
    "    'RFed': rfed\n",
    "}\n",
    "\n",
    "\n",
    "# -------------------\n",
    "# Federated Training Function\n",
    "# -------------------\n",
    "def federated_training(rounds, epochs, client_fraction, attack_types, attack_ratios, defense_method, iid, alpha, device):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    client_data = partition_data(mnist_train, num_clients=100, alpha=alpha, iid=iid)\n",
    "\n",
    "    global malicious_client_ids\n",
    "    malicious_client_ids = []\n",
    "    if attack_types and attack_ratios:\n",
    "        client_data = apply_combined_targeted_attacks(client_data, attack_types, attack_ratios)\n",
    "\n",
    "    global_model = SimpleCNN().to(device)\n",
    "    global_weights = global_model.state_dict()\n",
    "    prev_weights = {k: v.clone() for k, v in global_weights.items()}\n",
    "\n",
    "    results = []\n",
    "    test_loader = DataLoader(mnist_test, batch_size=128, shuffle=False)\n",
    "\n",
    "    for rnd in range(rounds):\n",
    "        global detected_malicious_ids, aggregation_weights\n",
    "        detected_malicious_ids = []\n",
    "        aggregation_weights = []\n",
    "\n",
    "        selected_clients = random.sample(list(client_data.keys()), int(client_fraction * len(client_data)))\n",
    "        local_updates = []\n",
    "        total_train_loss = 0.0\n",
    "        total_train_correct = 0\n",
    "        total_train_samples = 0\n",
    "\n",
    "        for cid in selected_clients:\n",
    "            local_model = SimpleCNN().to(device)\n",
    "            local_model.load_state_dict(global_weights)\n",
    "            optimizer = optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "            local_model.train()\n",
    "            for _ in range(epochs):\n",
    "                for x, y in client_data[cid]:\n",
    "                    x, y = x.to(device), y.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    output = local_model(x)\n",
    "                    loss = F.cross_entropy(output, y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    total_train_loss += loss.item() * x.size(0)\n",
    "                    total_train_correct += (output.argmax(1) == y).sum().item()\n",
    "                    total_train_samples += x.size(0)\n",
    "\n",
    "            local_updates.append({k: v.cpu().detach() for k, v in local_model.state_dict().items()})\n",
    "\n",
    "        if defense_method in AGGREGATION_FUNCTIONS:\n",
    "            global_weights = AGGREGATION_FUNCTIONS[defense_method](local_updates)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown defense method: {defense_method}\")\n",
    "\n",
    "        global_model.load_state_dict(global_weights)\n",
    "\n",
    "        acc, prec, rec, f1, fpr, tpr, test_loss = evaluate_model(global_model, test_loader, device)\n",
    "        attack_acc, exclusion_rate = detect_malicious_clients(malicious_client_ids, detected_malicious_ids)\n",
    "        entropy = compute_entropy(aggregation_weights) if aggregation_weights else 0.0\n",
    "        drift = compute_model_drift(prev_weights, global_weights)\n",
    "        prev_weights = {k: v.clone() for k, v in global_weights.items()}\n",
    "\n",
    "        avg_train_loss = total_train_loss / total_train_samples if total_train_samples > 0 else 0.0\n",
    "        avg_train_acc = total_train_correct / total_train_samples if total_train_samples > 0 else 0.0\n",
    "\n",
    "        results.append({\n",
    "            'Round': rnd + 1, 'Accuracy': acc, 'Precision': prec, 'Recall': rec, 'F1': f1,\n",
    "            'FPR': fpr, 'TPR': tpr, 'TestLoss': test_loss,\n",
    "            'TrainLoss': avg_train_loss, 'TrainAccuracy': avg_train_acc,\n",
    "            'AttackDetectAcc': attack_acc, 'ExclusionRate': exclusion_rate,\n",
    "            'Entropy': entropy, 'ModelDrift': drift\n",
    "        })\n",
    "\n",
    "        print(f\"Round {rnd + 1}: Acc={acc:.4f}, Prec={prec:.4f}, Rec={rec:.4f}, F1={f1:.4f}, \"\n",
    "              f\"FPR={fpr:.4f}, TPR={tpr:.4f}, TestLoss={test_loss:.4f}, TrainLoss={avg_train_loss:.4f}, \"\n",
    "              f\"TrainAcc={avg_train_acc:.4f}, AttackDetectAcc={attack_acc:.4f}, \"\n",
    "              f\"ExclusionRate={exclusion_rate:.4f}, Entropy={entropy:.4f}, Drift={drift:.4f}\")\n",
    "\n",
    "    pd.DataFrame(results).to_csv('fl_full_results.csv', index=False)\n",
    "    print(\"Federated training completed. Metrics saved to fl_full_results.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60cea8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1: Acc=0.1392, Prec=0.0316, Rec=0.1464, F1=0.0516, FPR=0.0948, TPR=0.1464, TestLoss=2.3045, TrainLoss=1.7709, TrainAcc=0.3901, AttackDetectAcc=0.0000, ExclusionRate=0.0000, Entropy=2.3026, Drift=0.0590\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Execute federated training\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m federated_training(\n\u001b[0;32m      8\u001b[0m     rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m      9\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     10\u001b[0m     client_fraction\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m     11\u001b[0m     attack_types\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_flipping\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     12\u001b[0m     attack_ratios\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_flipping\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.2\u001b[39m},\n\u001b[0;32m     13\u001b[0m     defense_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMedian\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     14\u001b[0m     iid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     15\u001b[0m     alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m,\n\u001b[0;32m     16\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# End the timer\u001b[39;00m\n\u001b[0;32m     20\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[1;32mIn[3], line 647\u001b[0m, in \u001b[0;36mfederated_training\u001b[1;34m(rounds, epochs, client_fraction, attack_types, attack_ratios, defense_method, iid, alpha, device)\u001b[0m\n\u001b[0;32m    645\u001b[0m x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    646\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 647\u001b[0m output \u001b[38;5;241m=\u001b[39m local_model(x)\n\u001b[0;32m    648\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(output, y)\n\u001b[0;32m    649\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[3], line 33\u001b[0m, in \u001b[0;36mSimpleCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     31\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)\n\u001b[0;32m     32\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m---> 33\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmax_pool2d(x, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     34\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n\u001b[0;32m     35\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_jit_internal.py:484\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_false(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:782\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    781\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[1;32m--> 782\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmax_pool2d(\u001b[38;5;28minput\u001b[39m, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Execute federated training\n",
    "federated_training(\n",
    "    rounds=5,\n",
    "    epochs=2,\n",
    "    client_fraction=0.1,\n",
    "    attack_types=['label_flipping'],\n",
    "    attack_ratios={'label_flipping': 0.2},\n",
    "    defense_method='Median',\n",
    "    iid=False,\n",
    "    alpha=0.3,\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Compute elapsed time in minutes and seconds\n",
    "elapsed = end_time - start_time\n",
    "minutes = int(elapsed // 60)\n",
    "seconds = int(elapsed % 60)\n",
    "\n",
    "print(f\"\\nTotal Training Time: {minutes} minutes and {seconds} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371cee31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e487364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# -------------------------\n",
    "# Global Attack Configuration\n",
    "# -------------------------\n",
    "TARGET_LABEL_FOR_POISONING = 0\n",
    "TARGET_LABEL_FLIP_MAP = {1: 7, 3: 8}  # Only flip 1→7, 3→8\n",
    "FEATURE_MANIPULATION_TARGET_CLASS = 8\n",
    "malicious_client_ids = []\n",
    "\n",
    "# -------------------------\n",
    "# Targeted Label Flipping Attack\n",
    "# -------------------------\n",
    "def apply_targeted_label_flipping_attack(loader):\n",
    "    \"\"\"\n",
    "    Flip labels of specific classes (1 → 7, 3 → 8) in the MNIST dataset.\n",
    "    Other labels are left unchanged.\n",
    "    \"\"\"\n",
    "    attacked = []\n",
    "    for x, y in loader:\n",
    "        y_flipped = torch.tensor([\n",
    "            TARGET_LABEL_FLIP_MAP.get(int(label), int(label)) for label in y\n",
    "        ])\n",
    "        attacked.extend([(x[i], y_flipped[i]) for i in range(len(x))])\n",
    "    x_tensor = torch.stack([d[0] for d in attacked])\n",
    "    y_tensor = torch.tensor([d[1] for d in attacked])\n",
    "    return DataLoader(TensorDataset(x_tensor, y_tensor), batch_size=32, shuffle=True)\n",
    "\n",
    "# -------------------------\n",
    "# Targeted Feature Manipulation Attack\n",
    "# -------------------------\n",
    "def apply_targeted_feature_manipulation_attack(loader):\n",
    "    \"\"\"\n",
    "    Adds structured noise only to images of label 8.\n",
    "    Other inputs remain unchanged.\n",
    "    \"\"\"\n",
    "    attacked = []\n",
    "    for x, y in loader:\n",
    "        x_attacked = x.clone()\n",
    "        for i in range(len(x)):\n",
    "            if y[i] == FEATURE_MANIPULATION_TARGET_CLASS:\n",
    "                noise = torch.randn_like(x[i]) * 0.5\n",
    "                mask = (torch.rand_like(x[i]) > 0.6).float()\n",
    "                x_attacked[i] = x[i] * mask + noise * (1 - mask)\n",
    "        attacked.extend([(x_attacked[i], y[i]) for i in range(len(x))])\n",
    "    x_tensor = torch.stack([d[0] for d in attacked])\n",
    "    y_tensor = torch.tensor([d[1] for d in attacked])\n",
    "    return DataLoader(TensorDataset(x_tensor, y_tensor), batch_size=32, shuffle=True)\n",
    "\n",
    "# -------------------------\n",
    "# Targeted Data Poisoning Attack (Backdoor)\n",
    "# -------------------------\n",
    "def apply_targeted_poisoning_attack(loader):\n",
    "    \"\"\"\n",
    "    Adds a white square trigger to each image and changes its label to the target (0).\n",
    "    \"\"\"\n",
    "    attacked = []\n",
    "    for x, y in loader:\n",
    "        x_poisoned = torch.clamp(x + 0.2, 0, 1)  # Global intensity shift\n",
    "        trigger = torch.zeros_like(x_poisoned)\n",
    "        trigger[:, :, -3:, -3:] = 1.0  # Add white square at bottom-right corner\n",
    "        x_poisoned = torch.clamp(x_poisoned + trigger, 0, 1)\n",
    "        y_target = torch.full_like(y, TARGET_LABEL_FOR_POISONING)\n",
    "        attacked.extend([(x_poisoned[i], y_target[i]) for i in range(len(x))])\n",
    "    x_tensor = torch.stack([d[0] for d in attacked])\n",
    "    y_tensor = torch.tensor([d[1] for d in attacked])\n",
    "    return DataLoader(TensorDataset(x_tensor, y_tensor), batch_size=32, shuffle=True)\n",
    "\n",
    "# -------------------------\n",
    "# Combined Targeted Attack Function\n",
    "# -------------------------\n",
    "def apply_combined_targeted_attacks(client_data, attack_types, attack_ratios):\n",
    "    \"\"\"\n",
    "    Applies selected targeted attacks to a subset of clients based on the given ratios.\n",
    "    :param client_data: dictionary of {client_id: DataLoader}\n",
    "    :param attack_types: list of attacks to apply: ['label_flipping', 'feature_manipulation', 'poisoning']\n",
    "    :param attack_ratios: dictionary of ratios per attack type\n",
    "    :return: modified client_data with attacked data for selected clients\n",
    "    \"\"\"\n",
    "    attack_specific_ids = {}\n",
    "    total_clients = list(client_data.keys())\n",
    "    num_clients = len(total_clients)\n",
    "\n",
    "    # Assign clients to attacks\n",
    "    for attack in attack_types:\n",
    "        count = int(num_clients * attack_ratios.get(attack, 0))\n",
    "        attack_specific_ids[attack] = random.sample(total_clients, count)\n",
    "\n",
    "    # Merge all malicious client IDs\n",
    "    global malicious_client_ids\n",
    "    malicious_client_ids = list(set().union(*attack_specific_ids.values()))\n",
    "\n",
    "    # Apply attack on selected clients\n",
    "    for client_id in malicious_client_ids:\n",
    "        loader = client_data[client_id]\n",
    "        if 'label_flipping' in attack_types and client_id in attack_specific_ids.get('label_flipping', []):\n",
    "            loader = apply_targeted_label_flipping_attack(loader)\n",
    "        if 'feature_manipulation' in attack_types and client_id in attack_specific_ids.get('feature_manipulation', []):\n",
    "            loader = apply_targeted_feature_manipulation_attack(loader)\n",
    "        if 'poisoning' in attack_types and client_id in attack_specific_ids.get('poisoning', []):\n",
    "            loader = apply_targeted_poisoning_attack(loader)\n",
    "        client_data[client_id] = loader\n",
    "\n",
    "    return client_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2fd96f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294d654c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
