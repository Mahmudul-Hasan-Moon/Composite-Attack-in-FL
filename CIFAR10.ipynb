{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f28306f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# Full Implementation with Attack Selection, Percentage Control, and Real Defense Methods\n",
    "# -------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "import math\n",
    "\n",
    "# -------------------\n",
    "# CNN Model\n",
    "# -------------------\n",
    "# class SimpleCNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(SimpleCNN, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "#         self.dropout1 = nn.Dropout(0.25)\n",
    "#         self.fc1 = nn.Linear(9216, 128)\n",
    "#         self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = F.max_pool2d(x, 2)\n",
    "#         x = self.dropout1(x)\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         x = self.fc1(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "class CIFAR_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CIFAR_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "# -------------------\n",
    "# Data Partition (IID or non-IID using Dirichlet)\n",
    "# -------------------\n",
    "def partition_data(dataset, num_clients=100, alpha=0.5, iid=True):\n",
    "    if iid:\n",
    "        data_split = torch.utils.data.random_split(dataset, [len(dataset) // num_clients] * num_clients)\n",
    "    else:\n",
    "        labels = np.array(dataset.targets)\n",
    "        idx_by_class = [np.where(labels == i)[0] for i in range(10)]\n",
    "        data_split = [[] for _ in range(num_clients)]\n",
    "        for c, idx in enumerate(idx_by_class):\n",
    "            np.random.shuffle(idx)\n",
    "            proportions = np.random.dirichlet(np.repeat(alpha, num_clients))\n",
    "            proportions = (np.cumsum(proportions) * len(idx)).astype(int)[:-1]\n",
    "            splits = np.split(idx, proportions)\n",
    "            for client_id, split in enumerate(splits):\n",
    "                data_split[client_id].extend(split)\n",
    "        data_split = [Subset(dataset, idxs) for idxs in data_split]\n",
    "    return {i: DataLoader(data_split[i], batch_size=32, shuffle=True) for i in range(num_clients)}\n",
    "\n",
    "# -------------------\n",
    "# Attack Injection\n",
    "# -------------------\n",
    "malicious_client_ids = []\n",
    "\n",
    "def apply_label_flipping_attack(loader):\n",
    "    flip_map = {0:1, 1:0, 2:3, 3:2, 4:5, 5:4, 6:7, 7:6, 8:9, 9:8}\n",
    "    attacked = []\n",
    "    for x, y in loader:\n",
    "        y_flipped = torch.tensor([flip_map[int(label)] for label in y])\n",
    "        attacked.extend([(x[i], y_flipped[i]) for i in range(len(x))])\n",
    "    x_tensor = torch.stack([d[0] for d in attacked])\n",
    "    y_tensor = torch.tensor([d[1] for d in attacked])\n",
    "    return DataLoader(TensorDataset(x_tensor, y_tensor), batch_size=32, shuffle=True)\n",
    "\n",
    "def apply_feature_manipulation_attack(loader):\n",
    "    attacked = []\n",
    "    for x, y in loader:\n",
    "        # Stronger manipulation: add structured noise and masking\n",
    "        noise = torch.randn_like(x) * 0.7  # Increased noise factor\n",
    "        mask = (torch.rand_like(x) > 0.7).float()  # Random masking\n",
    "        x = x * mask + noise * (1 - mask)  # Apply noise and mask\n",
    "        attacked.extend([(x[i], y[i]) for i in range(len(x))])\n",
    "    x_tensor = torch.stack([d[0] for d in attacked])\n",
    "    y_tensor = torch.tensor([d[1] for d in attacked])\n",
    "    return DataLoader(TensorDataset(x_tensor, y_tensor), batch_size=32, shuffle=True)\n",
    "\n",
    "def apply_poisoning_attack(loader):\n",
    "    attacked = []\n",
    "    for x, y in loader:\n",
    "        # Complex poisoning: inject strong pixel shift and structured trigger pattern\n",
    "        x = torch.clamp(x + 0.3 * torch.ones_like(x), 0, 1)  # Shift intensities\n",
    "        trigger = torch.zeros_like(x)\n",
    "        trigger[:, :, -3:, -3:] = 1.0  # Add white square trigger at bottom-right\n",
    "        x = torch.clamp(x + trigger, 0, 1)\n",
    "        attacked.extend([(x[i], y[i]) for i in range(len(x))])\n",
    "    x_tensor = torch.stack([d[0] for d in attacked])\n",
    "    y_tensor = torch.tensor([d[1] for d in attacked])\n",
    "    return DataLoader(TensorDataset(x_tensor, y_tensor), batch_size=32, shuffle=True)\n",
    "\n",
    "# -------------------\n",
    "# Combined Attack Function\n",
    "# -------------------\n",
    "def apply_combined_attacks(client_data, attack_types, attack_ratios):\n",
    "    # Select malicious clients for each attack separately\n",
    "    attack_specific_ids = {}\n",
    "    total_clients = list(client_data.keys())\n",
    "    num_clients = len(total_clients)\n",
    "\n",
    "    for attack in attack_types:\n",
    "        count = int(num_clients * attack_ratios.get(attack, 0))\n",
    "        attack_specific_ids[attack] = random.sample(total_clients, count)\n",
    "\n",
    "    malicious_ids = list(set().union(*attack_specific_ids.values()))\n",
    "    global malicious_client_ids\n",
    "    malicious_client_ids = malicious_ids\n",
    "\n",
    "    for client_id in malicious_ids:\n",
    "        loader = client_data[client_id]\n",
    "        if 'label_flipping' in attack_types and client_id in attack_specific_ids.get('label_flipping', []):\n",
    "            loader = apply_label_flipping_attack(loader)\n",
    "        if 'feature_manipulation' in attack_types and client_id in attack_specific_ids.get('feature_manipulation', []):\n",
    "            loader = apply_feature_manipulation_attack(loader)\n",
    "        if 'poisoning' in attack_types and client_id in attack_specific_ids.get('poisoning', []):\n",
    "            loader = apply_poisoning_attack(loader)\n",
    "        client_data[client_id] = loader\n",
    "\n",
    "    return client_data\n",
    "\n",
    "# -------------------\n",
    "# Evaluation Function\n",
    "# -------------------\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            if isinstance(batch, (list, tuple)) and len(batch) == 2:\n",
    "                data, target = batch\n",
    "            else:\n",
    "                raise ValueError(\"Each batch in test_loader must be a (data, target) tuple.\")\n",
    "\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            total += target.size(0)\n",
    "            y_true.extend(target.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(10)))\n",
    "    tp = np.diag(cm)\n",
    "    fp = np.sum(cm, axis=0) - tp\n",
    "    fn = np.sum(cm, axis=1) - tp\n",
    "    tn = np.sum(cm) - (tp + fp + fn)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    fpr = np.mean(fp / (fp + tn + 1e-6))\n",
    "    tpr = np.mean(tp / (tp + fn + 1e-6))\n",
    "    test_loss /= total\n",
    "\n",
    "    return accuracy, precision, recall, f1, fpr, tpr, test_loss\n",
    "\n",
    "# -------------------\n",
    "# Local Training Function (with Loss/Accuracy Logging)\n",
    "# -------------------\n",
    "def local_train(model, train_loader, epochs, device):\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    for epoch in range(epochs):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        total_loss = 0\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * data.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += data.size(0)\n",
    "        avg_loss = total_loss / total\n",
    "        avg_acc = correct / total\n",
    "        print(f\"Local Epoch {epoch+1} - Loss: {avg_loss:.4f}, Accuracy: {avg_acc:.4f}\")\n",
    "    return model.state_dict()\n",
    "\n",
    "# -------------------\n",
    "# Additional Metric Functions\n",
    "# -------------------\n",
    "def compute_model_drift(prev_weights, new_weights):\n",
    "    return sum(torch.norm(prev_weights[k] - new_weights[k]) for k in prev_weights) / len(prev_weights)\n",
    "\n",
    "def compute_entropy(weights):\n",
    "    weights = np.array(weights)\n",
    "    return -np.sum(weights * np.log(weights + 1e-10))\n",
    "\n",
    "def detect_malicious_clients(true_ids, detected_ids):\n",
    "    true_set = set(true_ids)\n",
    "    detected_set = set(detected_ids)\n",
    "    tp = len(true_set & detected_set)\n",
    "    fn = len(true_set - detected_set)\n",
    "    fp = len(detected_set - true_set)\n",
    "    attack_detection_rate = tp / (tp + fn + 1e-6)\n",
    "    exclusion_rate = len(detected_set) / (len(true_set | detected_set) + 1e-6)\n",
    "    return attack_detection_rate, exclusion_rate\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "prev_global_model = {}\n",
    "# -------------------\n",
    "# Aggregation Functions (Full Implementations)\n",
    "# -------------------\n",
    "\n",
    "global detected_malicious_ids\n",
    "detected_malicious_ids = []\n",
    "global aggregation_weights\n",
    "aggregation_weights = []\n",
    "\n",
    "def fltrust(updates):\n",
    "    reference_model = updates[0]  # trusted server model, in real FLTrust, it should be fixed\n",
    "    scores = []\n",
    "    for update in updates:\n",
    "        cosine_sim = sum(torch.nn.functional.cosine_similarity(update[k].flatten(), reference_model[k].flatten(), dim=0) for k in update)\n",
    "        scores.append(cosine_sim / len(update))\n",
    "    weights = torch.softmax(torch.tensor(scores), dim=0)\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = [i for i, w in enumerate(weights) if w < 0.01]\n",
    "    aggregation_weights = weights.tolist()\n",
    "    return {k: sum(weights[i] * updates[i][k] for i in range(len(updates))) for k in updates[0]}\n",
    "\n",
    "def flame(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    norms = [sum(torch.norm(u[k]) for k in u) for u in updates]\n",
    "    weights = torch.softmax(-torch.tensor(norms), dim=0)\n",
    "    aggregation_weights = weights.tolist()\n",
    "    norms = [sum(torch.norm(u[k]) for k in u) for u in updates]\n",
    "    weights = torch.softmax(-torch.tensor(norms), dim=0)\n",
    "    return {k: sum(weights[i] * updates[i][k] for i in range(len(updates))) for k in updates[0]}\n",
    "\n",
    "def flcert(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    aggregation_weights = []\n",
    "\n",
    "    # Step 1: Compute the mean update (global reference)\n",
    "    avg = fedavg(updates)\n",
    "\n",
    "    # Step 2: Compute cosine similarity between each update and the global mean\n",
    "    cosine_similarities = []\n",
    "    for u in updates:\n",
    "        sim = torch.stack([F.cosine_similarity(u[k].flatten(), avg[k].flatten(), dim=0) for k in u]).mean()\n",
    "        cosine_similarities.append(sim.item())\n",
    "\n",
    "    # Step 3: Thresholding (e.g., keep clients above 0.5 similarity)\n",
    "    threshold = 0.5\n",
    "    filtered_indices = [i for i, sim in enumerate(cosine_similarities) if sim >= threshold]\n",
    "    detected_malicious_ids = [i for i in range(len(updates)) if i not in filtered_indices]\n",
    "\n",
    "    if filtered_indices:\n",
    "        filtered_updates = [updates[i] for i in filtered_indices]\n",
    "        aggregation_weights = [1.0 / len(filtered_indices) if i in filtered_indices else 0.0 for i in range(len(updates))]\n",
    "        return fedavg(filtered_updates)\n",
    "    else:\n",
    "        aggregation_weights = [0.0] * len(updates)\n",
    "        return avg\n",
    "    avg = fedavg(updates)\n",
    "    filtered = [u for u in updates if sum(torch.norm(u[k] - avg[k]) for k in u) < 5.0]\n",
    "    return fedavg(filtered) if filtered else avg\n",
    "\n",
    "def foolsgold(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    gradients = [torch.cat([p.flatten() for p in u.values()]) for u in updates]\n",
    "    n = len(gradients)\n",
    "    sim = torch.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                sim[i][j] = torch.nn.functional.cosine_similarity(gradients[i], gradients[j], dim=0)\n",
    "    max_sim = sim.max(dim=1).values\n",
    "    weights = 1.0 - max_sim\n",
    "    weights = weights / weights.sum()\n",
    "    aggregation_weights = weights.tolist()\n",
    "    return {k: sum(weights[i] * updates[i][k] for i in range(n)) for k in updates[0]}\n",
    "\n",
    "def rfa(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "\n",
    "    # Step 1: Flatten updates for geometric median computation\n",
    "    vectors = [torch.cat([u[k].flatten() for k in sorted(u.keys())]) for u in updates]\n",
    "\n",
    "    # Step 2: Initialize with coordinate-wise median\n",
    "    stacked = torch.stack(vectors)\n",
    "    median = torch.median(stacked, dim=0)[0]\n",
    "\n",
    "    # Step 3: Iterative Weiszfeld algorithm\n",
    "    for _ in range(5):\n",
    "        distances = torch.stack([torch.norm(v - median) for v in vectors]) + 1e-10\n",
    "        weights = 1.0 / distances\n",
    "        weights /= weights.sum()\n",
    "        median = sum(weights[i] * vectors[i] for i in range(len(vectors)))\n",
    "\n",
    "    # Step 4: Unflatten median back to model format\n",
    "    example = updates[0]\n",
    "    aggregated = {}\n",
    "    pointer = 0\n",
    "    for k in sorted(example.keys()):\n",
    "        shape = example[k].shape\n",
    "        numel = example[k].numel()\n",
    "        aggregated[k] = median[pointer:pointer + numel].view(shape)\n",
    "        pointer += numel\n",
    "\n",
    "    # Mark clients with high distance as malicious\n",
    "    detected_malicious_ids = [i for i in range(len(vectors)) if torch.norm(vectors[i] - median) > 2.0]\n",
    "    aggregation_weights = [1.0 / len(vectors)] * len(vectors)\n",
    "\n",
    "    return aggregated\n",
    "    mean_update = fedavg(updates)\n",
    "    filtered = [u for u in updates if sum(torch.norm(u[k] - mean_update[k]) for k in u) < 3.0]\n",
    "    return fedavg(filtered) if filtered else mean_update\n",
    "\n",
    "def auror(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "\n",
    "    # Step 1: Flatten all updates\n",
    "    flat_updates = [torch.cat([u[k].flatten() for k in sorted(u.keys())]) for u in updates]\n",
    "\n",
    "    # Step 2: Compute pairwise L2 distances\n",
    "    num_clients = len(flat_updates)\n",
    "    distances = torch.zeros((num_clients, num_clients))\n",
    "    for i in range(num_clients):\n",
    "        for j in range(i + 1, num_clients):\n",
    "            dist = torch.norm(flat_updates[i] - flat_updates[j]).item()\n",
    "            distances[i][j] = distances[j][i] = dist\n",
    "\n",
    "    # Step 3: Compute score based on distance to k nearest neighbors\n",
    "    k = max(1, num_clients // 10)\n",
    "    scores = []\n",
    "    for i in range(num_clients):\n",
    "        sorted_dists = torch.sort(distances[i])[0]\n",
    "        scores.append(sorted_dists[1:k + 1].mean().item())  # Exclude self-distance\n",
    "\n",
    "    # Step 4: Select clients with lowest scores (most consistent)\n",
    "    sorted_ids = sorted(range(num_clients), key=lambda i: scores[i])\n",
    "    selected_ids = sorted_ids[:num_clients // 2]\n",
    "\n",
    "    detected_malicious_ids = [i for i in range(num_clients) if i not in selected_ids]\n",
    "    aggregation_weights = [1.0 / len(selected_ids) if i in selected_ids else 0.0 for i in range(num_clients)]\n",
    "\n",
    "    return fedavg([updates[i] for i in selected_ids])\n",
    "    #global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    scores = []\n",
    "    for i in range(len(updates)):\n",
    "        sim_sum = sum(torch.sum((updates[i][k] - updates[j][k])**2) for j in range(len(updates)) if j != i for k in updates[i])\n",
    "        scores.append(sim_sum)\n",
    "    best_ids = sorted(range(len(scores)), key=lambda i: scores[i])[:len(updates)//2]\n",
    "    return fedavg([updates[i] for i in best_ids])\n",
    "\n",
    "def bulyan(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    aggregation_weights = []\n",
    "\n",
    "    # Step 1: Compute distances between updates\n",
    "    n = len(updates)\n",
    "    distances = torch.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            d = sum(torch.norm(updates[i][k] - updates[j][k])**2 for k in updates[i])\n",
    "            distances[i][j] = distances[j][i] = d\n",
    "\n",
    "    # Step 2: Krum candidate selection (select n - 2f - 2 clients)\n",
    "    f = max(1, n // 10)\n",
    "    krum_candidates = []\n",
    "    for i in range(n):\n",
    "        sorted_dists = sorted(distances[i][j] for j in range(n) if j != i)\n",
    "        krum_score = sum(sorted_dists[:n - f - 2])\n",
    "        krum_candidates.append((i, krum_score))\n",
    "    krum_candidates.sort(key=lambda x: x[1])\n",
    "    selected_ids = [idx for idx, _ in krum_candidates[:n - 2 * f]]\n",
    "\n",
    "    # Step 3: For each parameter, collect values and perform trimmed mean\n",
    "    trimmed_updates = [updates[i] for i in selected_ids]\n",
    "    aggregation_weights = [1.0 / len(trimmed_updates) if i in selected_ids else 0.0 for i in range(n)]\n",
    "    detected_malicious_ids = [i for i in range(n) if i not in selected_ids]\n",
    "\n",
    "    return trimmed_mean(trimmed_updates)\n",
    "\n",
    "def rfed(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "\n",
    "    # Flatten each update and compute local variances\n",
    "    flat_updates = [torch.cat([u[k].flatten() for k in sorted(u.keys())]) for u in updates]\n",
    "    stacked = torch.stack(flat_updates)\n",
    "    feature_variance = torch.var(stacked, dim=0)\n",
    "\n",
    "    # Compute Mahalanobis-like distance from global variance\n",
    "    distances = []\n",
    "    for i in range(len(flat_updates)):\n",
    "        diff = flat_updates[i] - stacked.mean(dim=0)\n",
    "        dist = (diff ** 2 / (feature_variance + 1e-6)).mean()\n",
    "        distances.append(dist)\n",
    "\n",
    "    weights = torch.softmax(-torch.tensor(distances), dim=0)\n",
    "    aggregation_weights = weights.tolist()\n",
    "\n",
    "    # Aggregate\n",
    "    aggregated = {}\n",
    "    for k in updates[0].keys():\n",
    "        aggregated[k] = sum(weights[i] * updates[i][k] for i in range(len(updates)))\n",
    "\n",
    "    return aggregated\n",
    "\n",
    "def fedavg(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    aggregation_weights = [1.0 / len(updates)] * len(updates)\n",
    "    return {k: torch.stack([u[k] for u in updates], 0).mean(0) for k in updates[0].keys()}\n",
    "\n",
    "def median(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    aggregation_weights = [1.0 / len(updates)] * len(updates)\n",
    "    return {k: torch.median(torch.stack([u[k] for u in updates], 0), dim=0)[0] for k in updates[0].keys()}\n",
    "\n",
    "def trimmed_mean(updates, beta=0.1):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    aggregation_weights = [1.0 / len(updates)] * len(updates)\n",
    "    n = len(updates)\n",
    "    trim = int(n * beta)\n",
    "    agg = {}\n",
    "    for k in updates[0].keys():\n",
    "        stacked = torch.stack([u[k] for u in updates], dim=0)\n",
    "        sorted_vals, _ = stacked.sort(dim=0)\n",
    "        trimmed = sorted_vals[trim:n-trim]\n",
    "        agg[k] = trimmed.mean(dim=0)\n",
    "    return agg\n",
    "\n",
    "def krum(updates, f=1):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    aggregation_weights = [1.0 / len(updates)] * len(updates)\n",
    "    scores = []\n",
    "    for i, ui in enumerate(updates):\n",
    "        dists = []\n",
    "        for j, uj in enumerate(updates):\n",
    "            if i != j:\n",
    "                dist = sum(torch.norm(ui[k] - uj[k])**2 for k in ui)\n",
    "                dists.append(dist)\n",
    "        dists.sort()\n",
    "        scores.append((i, sum(dists[:len(updates)-f-2])) if len(dists) > f+2 else (i, float('inf')))\n",
    "    selected = min(scores, key=lambda x: x[1])[0]\n",
    "    return updates[selected]\n",
    "\n",
    "AGGREGATION_FUNCTIONS = {\n",
    "    'FedAVG': fedavg,\n",
    "    'TrimmedMean': trimmed_mean,\n",
    "    'Median': median,\n",
    "    'Krum': krum,\n",
    "    'FLTrust': fltrust,\n",
    "    'FLAME': flame,\n",
    "    'FLCert': flcert,\n",
    "    'FoolsGold': foolsgold,\n",
    "    'RFA': rfa,\n",
    "    'Auror': auror,\n",
    "    'Bulyan': bulyan,\n",
    "    'RFed': rfed\n",
    "}\n",
    "\n",
    "# -------------------\n",
    "# Federated Training Function\n",
    "# -------------------\n",
    "# def federated_training(rounds, epochs, client_fraction, attack_types, attack_ratios, defense_method, iid, alpha, device):\n",
    "#     transform = transforms.Compose([transforms.ToTensor()])\n",
    "#     mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "#     mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "#     client_data = partition_data(mnist_train, num_clients=100, alpha=alpha, iid=iid)\n",
    "\n",
    "#     if attack_types and attack_ratios:\n",
    "#         client_data = apply_combined_attacks(client_data, attack_types, attack_ratios)\n",
    "\n",
    "#     global_model = SimpleCNN().to(device)\n",
    "#     global_weights = global_model.state_dict()\n",
    "#     prev_weights = {k: v.clone() for k, v in global_weights.items()}\n",
    "\n",
    "#     results = []\n",
    "#     test_loader = DataLoader(mnist_test, batch_size=128, shuffle=False)\n",
    "\n",
    "#     for rnd in range(rounds):\n",
    "#         selected_clients = random.sample(list(client_data.keys()), int(client_fraction * len(client_data)))\n",
    "#         local_updates = []\n",
    "#         total_train_loss = 0.0\n",
    "#         total_train_correct = 0\n",
    "#         total_train_samples = 0\n",
    "\n",
    "#         for cid in selected_clients:\n",
    "#             local_model = SimpleCNN().to(device)\n",
    "#             local_model.load_state_dict(global_weights)\n",
    "#             optimizer = optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "#             local_model.train()\n",
    "#             for _ in range(epochs):\n",
    "#                 for x, y in client_data[cid]:\n",
    "#                     x, y = x.to(device), y.to(device)\n",
    "#                     optimizer.zero_grad()\n",
    "#                     output = local_model(x)\n",
    "#                     loss = F.cross_entropy(output, y)\n",
    "#                     loss.backward()\n",
    "#                     optimizer.step()\n",
    "\n",
    "#                     total_train_loss += loss.item() * x.size(0)\n",
    "#                     total_train_correct += (output.argmax(1) == y).sum().item()\n",
    "#                     total_train_samples += x.size(0)\n",
    "\n",
    "#             local_updates.append({k: v.cpu().detach() for k, v in local_model.state_dict().items()})\n",
    "\n",
    "#         if defense_method in AGGREGATION_FUNCTIONS:\n",
    "#             global_weights = AGGREGATION_FUNCTIONS[defense_method](local_updates)\n",
    "#         else:\n",
    "#             raise ValueError(f\"Unknown defense method: {defense_method}\")\n",
    "\n",
    "#         global_model.load_state_dict(global_weights)\n",
    "\n",
    "#         acc, prec, rec, f1, fpr, tpr, test_loss = evaluate_model(global_model, test_loader, device)\n",
    "#         attack_acc, exclusion_rate = detect_malicious_clients(malicious_client_ids, detected_malicious_ids)\n",
    "#         entropy = compute_entropy(aggregation_weights) if 'aggregation_weights' in globals() else 0.0\n",
    "#         drift = compute_model_drift(prev_weights, global_weights)\n",
    "#         prev_weights = {k: v.clone() for k, v in global_weights.items()}\n",
    "\n",
    "#         avg_train_loss = total_train_loss / total_train_samples if total_train_samples > 0 else 0.0\n",
    "#         avg_train_acc = total_train_correct / total_train_samples if total_train_samples > 0 else 0.0\n",
    "\n",
    "#         results.append({\n",
    "#             'Round': rnd + 1, 'Accuracy': acc, 'Precision': prec, 'Recall': rec, 'F1': f1,\n",
    "#             'FPR': fpr, 'TPR': tpr, 'TestLoss': test_loss,\n",
    "#             'TrainLoss': avg_train_loss, 'TrainAccuracy': avg_train_acc,\n",
    "#             'AttackDetectAcc': attack_acc, 'ExclusionRate': exclusion_rate,\n",
    "#             'Entropy': entropy, 'ModelDrift': drift\n",
    "#         })\n",
    "\n",
    "#         print(f\"Round {rnd + 1}: Acc={acc:.4f}, Prec={prec:.4f}, Rec={rec:.4f}, F1={f1:.4f}, \"\n",
    "#               f\"FPR={fpr:.4f}, TPR={tpr:.4f}, TestLoss={test_loss:.4f}, TrainLoss={avg_train_loss:.4f}, \"\n",
    "#               f\"TrainAcc={avg_train_acc:.4f}, AttackDetectAcc={attack_acc:.4f}, \"\n",
    "#               f\"ExclusionRate={exclusion_rate:.4f}, Entropy={entropy:.4f}, Drift={drift:.4f}\")\n",
    "\n",
    "#     pd.DataFrame(results).to_csv('fl_full_results.csv', index=False)\n",
    "#     print(\"Federated training completed. Metrics saved to fl_full_results.csv\")\n",
    "\n",
    "# -------------------\n",
    "# Federated Training Function\n",
    "# -------------------\n",
    "def federated_training(rounds, epochs, client_fraction, attack_types, attack_ratios, defense_method, iid, alpha, device):\n",
    "    cifar_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # normalize CIFAR-10\n",
    "    ])\n",
    "    cifar_train = datasets.CIFAR10(root='./data', train=True, download=True, transform=cifar_transform)\n",
    "    cifar_test = datasets.CIFAR10(root='./data', train=False, download=True, transform=cifar_transform)\n",
    "\n",
    "#     transform = transforms.Compose([transforms.ToTensor()])\n",
    "#     mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "#     mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    #client_data = partition_data(mnist_train, num_clients=100, alpha=alpha, iid=iid)\n",
    "    client_data = partition_data(cifar_train, num_clients=100, alpha=alpha, iid=iid)\n",
    "    test_loader = DataLoader(cifar_test, batch_size=128, shuffle=False)\n",
    "\n",
    "\n",
    "    global malicious_client_ids\n",
    "    malicious_client_ids = []\n",
    "    if attack_types and attack_ratios:\n",
    "        client_data = apply_combined_attacks(client_data, attack_types, attack_ratios)\n",
    "\n",
    "    #global_model = SimpleCNN().to(device)\n",
    "    global_model = CIFAR_CNN().to(device)\n",
    "    global_weights = global_model.state_dict()\n",
    "    prev_weights = {k: v.clone() for k, v in global_weights.items()}\n",
    "\n",
    "    results = []\n",
    "    test_loader = DataLoader(cifar_test, batch_size=128, shuffle=False)\n",
    "\n",
    "    for rnd in range(rounds):\n",
    "        global detected_malicious_ids, aggregation_weights\n",
    "        detected_malicious_ids = []\n",
    "        aggregation_weights = []\n",
    "\n",
    "        selected_clients = random.sample(list(client_data.keys()), int(client_fraction * len(client_data)))\n",
    "        local_updates = []\n",
    "        total_train_loss = 0.0\n",
    "        total_train_correct = 0\n",
    "        total_train_samples = 0\n",
    "\n",
    "        for cid in selected_clients:\n",
    "            #local_model = SimpleCNN().to(device)\n",
    "            local_model = CIFAR_CNN().to(device)\n",
    "            local_model.load_state_dict(global_weights)\n",
    "            optimizer = optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "            local_model.train()\n",
    "            for _ in range(epochs):\n",
    "                for x, y in client_data[cid]:\n",
    "                    x, y = x.to(device), y.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    output = local_model(x)\n",
    "                    loss = F.cross_entropy(output, y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    total_train_loss += loss.item() * x.size(0)\n",
    "                    total_train_correct += (output.argmax(1) == y).sum().item()\n",
    "                    total_train_samples += x.size(0)\n",
    "\n",
    "            local_updates.append({k: v.cpu().detach() for k, v in local_model.state_dict().items()})\n",
    "\n",
    "        if defense_method in AGGREGATION_FUNCTIONS:\n",
    "            global_weights = AGGREGATION_FUNCTIONS[defense_method](local_updates)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown defense method: {defense_method}\")\n",
    "\n",
    "        global_model.load_state_dict(global_weights)\n",
    "\n",
    "        acc, prec, rec, f1, fpr, tpr, test_loss = evaluate_model(global_model, test_loader, device)\n",
    "        attack_acc, exclusion_rate = detect_malicious_clients(malicious_client_ids, detected_malicious_ids)\n",
    "        entropy = compute_entropy(aggregation_weights) if aggregation_weights else 0.0\n",
    "        drift = compute_model_drift(prev_weights, global_weights)\n",
    "        prev_weights = {k: v.clone() for k, v in global_weights.items()}\n",
    "\n",
    "        avg_train_loss = total_train_loss / total_train_samples if total_train_samples > 0 else 0.0\n",
    "        avg_train_acc = total_train_correct / total_train_samples if total_train_samples > 0 else 0.0\n",
    "\n",
    "        results.append({\n",
    "            'Round': rnd + 1, 'Accuracy': acc, 'Precision': prec, 'Recall': rec, 'F1': f1,\n",
    "            'FPR': fpr, 'TPR': tpr, 'TestLoss': test_loss,\n",
    "            'TrainLoss': avg_train_loss, 'TrainAccuracy': avg_train_acc,\n",
    "            'AttackDetectAcc': attack_acc, 'ExclusionRate': exclusion_rate,\n",
    "            'Entropy': entropy, 'ModelDrift': drift\n",
    "        })\n",
    "\n",
    "        print(f\"Round {rnd + 1}: Acc={acc:.4f}, Prec={prec:.4f}, Rec={rec:.4f}, F1={f1:.4f}, \"\n",
    "              f\"FPR={fpr:.4f}, TPR={tpr:.4f}, TestLoss={test_loss:.4f}, TrainLoss={avg_train_loss:.4f}, \"\n",
    "              f\"TrainAcc={avg_train_acc:.4f}, AttackDetectAcc={attack_acc:.4f}, \"\n",
    "              f\"ExclusionRate={exclusion_rate:.4f}, Entropy={entropy:.4f}, Drift={drift:.4f}\")\n",
    "\n",
    "    pd.DataFrame(results).to_csv('fl_full_results.csv', index=False)\n",
    "    print(\"Federated training completed. Metrics saved to fl_full_results.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "646b873b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Round 1: Acc=0.1069, Prec=0.0293, Rec=0.1069, F1=0.0290, FPR=0.0992, TPR=0.1069, TestLoss=2.3040, TrainLoss=1.7162, TrainAcc=0.5048, AttackDetectAcc=0.0000, ExclusionRate=0.0000, Entropy=2.3026, Drift=0.0536\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Start the timer\u001b[39;00m\n\u001b[0;32m      4\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 6\u001b[0m federated_training(\n\u001b[0;32m      7\u001b[0m     rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m      8\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m      9\u001b[0m     client_fraction\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m     10\u001b[0m     attack_types\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_flipping\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_manipulation\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     11\u001b[0m     attack_ratios\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_flipping\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_manipulation\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.3\u001b[39m},\n\u001b[0;32m     12\u001b[0m     defense_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMedian\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     13\u001b[0m     iid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     14\u001b[0m     alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m,\n\u001b[0;32m     15\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     16\u001b[0m )\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# End the timer\u001b[39;00m\n\u001b[0;32m     19\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[1;32mIn[4], line 643\u001b[0m, in \u001b[0;36mfederated_training\u001b[1;34m(rounds, epochs, client_fraction, attack_types, attack_ratios, defense_method, iid, alpha, device)\u001b[0m\n\u001b[0;32m    641\u001b[0m output \u001b[38;5;241m=\u001b[39m local_model(x)\n\u001b[0;32m    642\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(output, y)\n\u001b[1;32m--> 643\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    644\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    646\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "federated_training(\n",
    "    rounds=5,\n",
    "    epochs=2,\n",
    "    client_fraction=0.1,\n",
    "    attack_types=['label_flipping', 'feature_manipulation'],\n",
    "    attack_ratios={'label_flipping': 0.2, 'feature_manipulation': 0.3},\n",
    "    defense_method='Median',\n",
    "    iid=False,\n",
    "    alpha=0.3,\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Compute elapsed time in minutes and seconds\n",
    "elapsed = end_time - start_time\n",
    "minutes = int(elapsed // 60)\n",
    "seconds = int(elapsed % 60)\n",
    "\n",
    "print(f\"\\nTotal Training Time: {minutes} minutes and {seconds} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a54ad4",
   "metadata": {},
   "source": [
    "### Targeted Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff04e0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# Full Implementation with Attack Selection, Percentage Control, and Real Defense Methods\n",
    "# -------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "import math\n",
    "\n",
    "# -------------------\n",
    "# CNN Model\n",
    "# -------------------\n",
    "# class SimpleCNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(SimpleCNN, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "#         self.dropout1 = nn.Dropout(0.25)\n",
    "#         self.fc1 = nn.Linear(9216, 128)\n",
    "#         self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = F.max_pool2d(x, 2)\n",
    "#         x = self.dropout1(x)\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         x = self.fc1(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "class CIFAR_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CIFAR_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "# -------------------\n",
    "# Data Partition (IID or non-IID using Dirichlet)\n",
    "# -------------------\n",
    "def partition_data(dataset, num_clients=100, alpha=0.5, iid=True):\n",
    "    if iid:\n",
    "        data_split = torch.utils.data.random_split(dataset, [len(dataset) // num_clients] * num_clients)\n",
    "    else:\n",
    "        labels = np.array(dataset.targets)\n",
    "        idx_by_class = [np.where(labels == i)[0] for i in range(10)]\n",
    "        data_split = [[] for _ in range(num_clients)]\n",
    "        for c, idx in enumerate(idx_by_class):\n",
    "            np.random.shuffle(idx)\n",
    "            proportions = np.random.dirichlet(np.repeat(alpha, num_clients))\n",
    "            proportions = (np.cumsum(proportions) * len(idx)).astype(int)[:-1]\n",
    "            splits = np.split(idx, proportions)\n",
    "            for client_id, split in enumerate(splits):\n",
    "                data_split[client_id].extend(split)\n",
    "        data_split = [Subset(dataset, idxs) for idxs in data_split]\n",
    "    return {i: DataLoader(data_split[i], batch_size=32, shuffle=True) for i in range(num_clients)}\n",
    "\n",
    "# -------------------------\n",
    "# Global Attack Configuration for CIFAR-10\n",
    "# -------------------------\n",
    "TARGET_LABEL_FLIP_MAP = {3: 5, 0: 2}  # Cat(3) → Dog(5), Airplane(0) → Bird(2)\n",
    "FEATURE_MANIPULATION_TARGET_CLASS = 5  # Dog\n",
    "TARGET_LABEL_FOR_POISONING = 9         # Truck\n",
    "malicious_client_ids = []\n",
    "\n",
    "# -------------------------\n",
    "# Targeted Label Flipping Attack (CIFAR-10)\n",
    "# -------------------------\n",
    "def apply_targeted_label_flipping_attack_cifar(loader):\n",
    "    \"\"\"\n",
    "    Flip specific CIFAR-10 labels: Cat → Dog, Airplane → Bird.\n",
    "    \"\"\"\n",
    "    attacked = []\n",
    "    for x, y in loader:\n",
    "        y_flipped = torch.tensor([\n",
    "            TARGET_LABEL_FLIP_MAP.get(int(label), int(label)) for label in y\n",
    "        ])\n",
    "        attacked.extend([(x[i], y_flipped[i]) for i in range(len(x))])\n",
    "    x_tensor = torch.stack([d[0] for d in attacked])\n",
    "    y_tensor = torch.tensor([d[1] for d in attacked])\n",
    "    return DataLoader(TensorDataset(x_tensor, y_tensor), batch_size=32, shuffle=True)\n",
    "\n",
    "# -------------------------\n",
    "# Targeted Feature Manipulation Attack (CIFAR-10)\n",
    "# -------------------------\n",
    "def apply_targeted_feature_manipulation_attack_cifar(loader):\n",
    "    \"\"\"\n",
    "    Add structured noise to samples with label 'Dog' (class 5).\n",
    "    \"\"\"\n",
    "    attacked = []\n",
    "    for x, y in loader:\n",
    "        x_attacked = x.clone()\n",
    "        for i in range(len(x)):\n",
    "            if y[i] == FEATURE_MANIPULATION_TARGET_CLASS:\n",
    "                noise = torch.randn_like(x[i]) * 0.4\n",
    "                mask = (torch.rand_like(x[i]) > 0.7).float()\n",
    "                x_attacked[i] = x[i] * mask + noise * (1 - mask)\n",
    "        attacked.extend([(x_attacked[i], y[i]) for i in range(len(x))])\n",
    "    x_tensor = torch.stack([d[0] for d in attacked])\n",
    "    y_tensor = torch.tensor([d[1] for d in attacked])\n",
    "    return DataLoader(TensorDataset(x_tensor, y_tensor), batch_size=32, shuffle=True)\n",
    "\n",
    "# -------------------------\n",
    "# Targeted Data Poisoning Attack (CIFAR-10)\n",
    "# -------------------------\n",
    "def apply_targeted_poisoning_attack_cifar(loader):\n",
    "    \"\"\"\n",
    "    Add white trigger to bottom-right and label all poisoned samples as Truck (class 9).\n",
    "    \"\"\"\n",
    "    attacked = []\n",
    "    for x, y in loader:\n",
    "        x_poisoned = torch.clamp(x + 0.1, 0, 1)  # subtle brightness shift\n",
    "        trigger = torch.zeros_like(x_poisoned)\n",
    "        trigger[:, :, -3:, -3:] = 1.0  # white square in bottom-right\n",
    "        x_poisoned = torch.clamp(x_poisoned + trigger, 0, 1)\n",
    "        y_target = torch.full_like(y, TARGET_LABEL_FOR_POISONING)\n",
    "        attacked.extend([(x_poisoned[i], y_target[i]) for i in range(len(x))])\n",
    "    x_tensor = torch.stack([d[0] for d in attacked])\n",
    "    y_tensor = torch.tensor([d[1] for d in attacked])\n",
    "    return DataLoader(TensorDataset(x_tensor, y_tensor), batch_size=32, shuffle=True)\n",
    "\n",
    "# -------------------------\n",
    "# Combined Targeted Attack Function (CIFAR-10)\n",
    "# -------------------------\n",
    "def apply_combined_targeted_attacks_cifar(client_data, attack_types, attack_ratios):\n",
    "    \"\"\"\n",
    "    Applies selected targeted attacks on CIFAR-10 data to a subset of clients.\n",
    "    \"\"\"\n",
    "    attack_specific_ids = {}\n",
    "    total_clients = list(client_data.keys())\n",
    "    num_clients = len(total_clients)\n",
    "\n",
    "    # Assign malicious clients\n",
    "    for attack in attack_types:\n",
    "        count = int(num_clients * attack_ratios.get(attack, 0))\n",
    "        attack_specific_ids[attack] = random.sample(total_clients, count)\n",
    "\n",
    "    global malicious_client_ids\n",
    "    malicious_client_ids = list(set().union(*attack_specific_ids.values()))\n",
    "\n",
    "    # Inject attacks per client\n",
    "    for client_id in malicious_client_ids:\n",
    "        loader = client_data[client_id]\n",
    "        if 'label_flipping' in attack_types and client_id in attack_specific_ids.get('label_flipping', []):\n",
    "            loader = apply_targeted_label_flipping_attack_cifar(loader)\n",
    "        if 'feature_manipulation' in attack_types and client_id in attack_specific_ids.get('feature_manipulation', []):\n",
    "            loader = apply_targeted_feature_manipulation_attack_cifar(loader)\n",
    "        if 'poisoning' in attack_types and client_id in attack_specific_ids.get('poisoning', []):\n",
    "            loader = apply_targeted_poisoning_attack_cifar(loader)\n",
    "        client_data[client_id] = loader\n",
    "\n",
    "    return client_data\n",
    "\n",
    "\n",
    "# -------------------\n",
    "# Evaluation Function\n",
    "# -------------------\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            if isinstance(batch, (list, tuple)) and len(batch) == 2:\n",
    "                data, target = batch\n",
    "            else:\n",
    "                raise ValueError(\"Each batch in test_loader must be a (data, target) tuple.\")\n",
    "\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            total += target.size(0)\n",
    "            y_true.extend(target.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(10)))\n",
    "    tp = np.diag(cm)\n",
    "    fp = np.sum(cm, axis=0) - tp\n",
    "    fn = np.sum(cm, axis=1) - tp\n",
    "    tn = np.sum(cm) - (tp + fp + fn)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    fpr = np.mean(fp / (fp + tn + 1e-6))\n",
    "    tpr = np.mean(tp / (tp + fn + 1e-6))\n",
    "    test_loss /= total\n",
    "\n",
    "    return accuracy, precision, recall, f1, fpr, tpr, test_loss\n",
    "\n",
    "# -------------------\n",
    "# Local Training Function (with Loss/Accuracy Logging)\n",
    "# -------------------\n",
    "def local_train(model, train_loader, epochs, device):\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    for epoch in range(epochs):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        total_loss = 0\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * data.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += data.size(0)\n",
    "        avg_loss = total_loss / total\n",
    "        avg_acc = correct / total\n",
    "        print(f\"Local Epoch {epoch+1} - Loss: {avg_loss:.4f}, Accuracy: {avg_acc:.4f}\")\n",
    "    return model.state_dict()\n",
    "\n",
    "# -------------------\n",
    "# Additional Metric Functions\n",
    "# -------------------\n",
    "def compute_model_drift(prev_weights, new_weights):\n",
    "    return sum(torch.norm(prev_weights[k] - new_weights[k]) for k in prev_weights) / len(prev_weights)\n",
    "\n",
    "def compute_entropy(weights):\n",
    "    weights = np.array(weights)\n",
    "    return -np.sum(weights * np.log(weights + 1e-10))\n",
    "\n",
    "def detect_malicious_clients(true_ids, detected_ids):\n",
    "    true_set = set(true_ids)\n",
    "    detected_set = set(detected_ids)\n",
    "    tp = len(true_set & detected_set)\n",
    "    fn = len(true_set - detected_set)\n",
    "    fp = len(detected_set - true_set)\n",
    "    attack_detection_rate = tp / (tp + fn + 1e-6)\n",
    "    exclusion_rate = len(detected_set) / (len(true_set | detected_set) + 1e-6)\n",
    "    return attack_detection_rate, exclusion_rate\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "prev_global_model = {}\n",
    "# -------------------\n",
    "# Aggregation Functions (Full Implementations)\n",
    "# -------------------\n",
    "\n",
    "global detected_malicious_ids\n",
    "detected_malicious_ids = []\n",
    "global aggregation_weights\n",
    "aggregation_weights = []\n",
    "\n",
    "def fltrust(updates):\n",
    "    reference_model = updates[0]  # trusted server model, in real FLTrust, it should be fixed\n",
    "    scores = []\n",
    "    for update in updates:\n",
    "        cosine_sim = sum(torch.nn.functional.cosine_similarity(update[k].flatten(), reference_model[k].flatten(), dim=0) for k in update)\n",
    "        scores.append(cosine_sim / len(update))\n",
    "    weights = torch.softmax(torch.tensor(scores), dim=0)\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = [i for i, w in enumerate(weights) if w < 0.01]\n",
    "    aggregation_weights = weights.tolist()\n",
    "    return {k: sum(weights[i] * updates[i][k] for i in range(len(updates))) for k in updates[0]}\n",
    "\n",
    "def flame(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    norms = [sum(torch.norm(u[k]) for k in u) for u in updates]\n",
    "    weights = torch.softmax(-torch.tensor(norms), dim=0)\n",
    "    aggregation_weights = weights.tolist()\n",
    "    norms = [sum(torch.norm(u[k]) for k in u) for u in updates]\n",
    "    weights = torch.softmax(-torch.tensor(norms), dim=0)\n",
    "    return {k: sum(weights[i] * updates[i][k] for i in range(len(updates))) for k in updates[0]}\n",
    "\n",
    "def flcert(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    aggregation_weights = []\n",
    "\n",
    "    # Step 1: Compute the mean update (global reference)\n",
    "    avg = fedavg(updates)\n",
    "\n",
    "    # Step 2: Compute cosine similarity between each update and the global mean\n",
    "    cosine_similarities = []\n",
    "    for u in updates:\n",
    "        sim = torch.stack([F.cosine_similarity(u[k].flatten(), avg[k].flatten(), dim=0) for k in u]).mean()\n",
    "        cosine_similarities.append(sim.item())\n",
    "\n",
    "    # Step 3: Thresholding (e.g., keep clients above 0.5 similarity)\n",
    "    threshold = 0.5\n",
    "    filtered_indices = [i for i, sim in enumerate(cosine_similarities) if sim >= threshold]\n",
    "    detected_malicious_ids = [i for i in range(len(updates)) if i not in filtered_indices]\n",
    "\n",
    "    if filtered_indices:\n",
    "        filtered_updates = [updates[i] for i in filtered_indices]\n",
    "        aggregation_weights = [1.0 / len(filtered_indices) if i in filtered_indices else 0.0 for i in range(len(updates))]\n",
    "        return fedavg(filtered_updates)\n",
    "    else:\n",
    "        aggregation_weights = [0.0] * len(updates)\n",
    "        return avg\n",
    "    avg = fedavg(updates)\n",
    "    filtered = [u for u in updates if sum(torch.norm(u[k] - avg[k]) for k in u) < 5.0]\n",
    "    return fedavg(filtered) if filtered else avg\n",
    "\n",
    "def foolsgold(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    gradients = [torch.cat([p.flatten() for p in u.values()]) for u in updates]\n",
    "    n = len(gradients)\n",
    "    sim = torch.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                sim[i][j] = torch.nn.functional.cosine_similarity(gradients[i], gradients[j], dim=0)\n",
    "    max_sim = sim.max(dim=1).values\n",
    "    weights = 1.0 - max_sim\n",
    "    weights = weights / weights.sum()\n",
    "    aggregation_weights = weights.tolist()\n",
    "    return {k: sum(weights[i] * updates[i][k] for i in range(n)) for k in updates[0]}\n",
    "\n",
    "def rfa(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "\n",
    "    # Step 1: Flatten updates for geometric median computation\n",
    "    vectors = [torch.cat([u[k].flatten() for k in sorted(u.keys())]) for u in updates]\n",
    "\n",
    "    # Step 2: Initialize with coordinate-wise median\n",
    "    stacked = torch.stack(vectors)\n",
    "    median = torch.median(stacked, dim=0)[0]\n",
    "\n",
    "    # Step 3: Iterative Weiszfeld algorithm\n",
    "    for _ in range(5):\n",
    "        distances = torch.stack([torch.norm(v - median) for v in vectors]) + 1e-10\n",
    "        weights = 1.0 / distances\n",
    "        weights /= weights.sum()\n",
    "        median = sum(weights[i] * vectors[i] for i in range(len(vectors)))\n",
    "\n",
    "    # Step 4: Unflatten median back to model format\n",
    "    example = updates[0]\n",
    "    aggregated = {}\n",
    "    pointer = 0\n",
    "    for k in sorted(example.keys()):\n",
    "        shape = example[k].shape\n",
    "        numel = example[k].numel()\n",
    "        aggregated[k] = median[pointer:pointer + numel].view(shape)\n",
    "        pointer += numel\n",
    "\n",
    "    # Mark clients with high distance as malicious\n",
    "    detected_malicious_ids = [i for i in range(len(vectors)) if torch.norm(vectors[i] - median) > 2.0]\n",
    "    aggregation_weights = [1.0 / len(vectors)] * len(vectors)\n",
    "\n",
    "    return aggregated\n",
    "    mean_update = fedavg(updates)\n",
    "    filtered = [u for u in updates if sum(torch.norm(u[k] - mean_update[k]) for k in u) < 3.0]\n",
    "    return fedavg(filtered) if filtered else mean_update\n",
    "\n",
    "def auror(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "\n",
    "    # Step 1: Flatten all updates\n",
    "    flat_updates = [torch.cat([u[k].flatten() for k in sorted(u.keys())]) for u in updates]\n",
    "\n",
    "    # Step 2: Compute pairwise L2 distances\n",
    "    num_clients = len(flat_updates)\n",
    "    distances = torch.zeros((num_clients, num_clients))\n",
    "    for i in range(num_clients):\n",
    "        for j in range(i + 1, num_clients):\n",
    "            dist = torch.norm(flat_updates[i] - flat_updates[j]).item()\n",
    "            distances[i][j] = distances[j][i] = dist\n",
    "\n",
    "    # Step 3: Compute score based on distance to k nearest neighbors\n",
    "    k = max(1, num_clients // 10)\n",
    "    scores = []\n",
    "    for i in range(num_clients):\n",
    "        sorted_dists = torch.sort(distances[i])[0]\n",
    "        scores.append(sorted_dists[1:k + 1].mean().item())  # Exclude self-distance\n",
    "\n",
    "    # Step 4: Select clients with lowest scores (most consistent)\n",
    "    sorted_ids = sorted(range(num_clients), key=lambda i: scores[i])\n",
    "    selected_ids = sorted_ids[:num_clients // 2]\n",
    "\n",
    "    detected_malicious_ids = [i for i in range(num_clients) if i not in selected_ids]\n",
    "    aggregation_weights = [1.0 / len(selected_ids) if i in selected_ids else 0.0 for i in range(num_clients)]\n",
    "\n",
    "    return fedavg([updates[i] for i in selected_ids])\n",
    "    #global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    scores = []\n",
    "    for i in range(len(updates)):\n",
    "        sim_sum = sum(torch.sum((updates[i][k] - updates[j][k])**2) for j in range(len(updates)) if j != i for k in updates[i])\n",
    "        scores.append(sim_sum)\n",
    "    best_ids = sorted(range(len(scores)), key=lambda i: scores[i])[:len(updates)//2]\n",
    "    return fedavg([updates[i] for i in best_ids])\n",
    "\n",
    "def bulyan(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    aggregation_weights = []\n",
    "\n",
    "    # Step 1: Compute distances between updates\n",
    "    n = len(updates)\n",
    "    distances = torch.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            d = sum(torch.norm(updates[i][k] - updates[j][k])**2 for k in updates[i])\n",
    "            distances[i][j] = distances[j][i] = d\n",
    "\n",
    "    # Step 2: Krum candidate selection (select n - 2f - 2 clients)\n",
    "    f = max(1, n // 10)\n",
    "    krum_candidates = []\n",
    "    for i in range(n):\n",
    "        sorted_dists = sorted(distances[i][j] for j in range(n) if j != i)\n",
    "        krum_score = sum(sorted_dists[:n - f - 2])\n",
    "        krum_candidates.append((i, krum_score))\n",
    "    krum_candidates.sort(key=lambda x: x[1])\n",
    "    selected_ids = [idx for idx, _ in krum_candidates[:n - 2 * f]]\n",
    "\n",
    "    # Step 3: For each parameter, collect values and perform trimmed mean\n",
    "    trimmed_updates = [updates[i] for i in selected_ids]\n",
    "    aggregation_weights = [1.0 / len(trimmed_updates) if i in selected_ids else 0.0 for i in range(n)]\n",
    "    detected_malicious_ids = [i for i in range(n) if i not in selected_ids]\n",
    "\n",
    "    return trimmed_mean(trimmed_updates)\n",
    "\n",
    "def rfed(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "\n",
    "    # Flatten each update and compute local variances\n",
    "    flat_updates = [torch.cat([u[k].flatten() for k in sorted(u.keys())]) for u in updates]\n",
    "    stacked = torch.stack(flat_updates)\n",
    "    feature_variance = torch.var(stacked, dim=0)\n",
    "\n",
    "    # Compute Mahalanobis-like distance from global variance\n",
    "    distances = []\n",
    "    for i in range(len(flat_updates)):\n",
    "        diff = flat_updates[i] - stacked.mean(dim=0)\n",
    "        dist = (diff ** 2 / (feature_variance + 1e-6)).mean()\n",
    "        distances.append(dist)\n",
    "\n",
    "    weights = torch.softmax(-torch.tensor(distances), dim=0)\n",
    "    aggregation_weights = weights.tolist()\n",
    "\n",
    "    # Aggregate\n",
    "    aggregated = {}\n",
    "    for k in updates[0].keys():\n",
    "        aggregated[k] = sum(weights[i] * updates[i][k] for i in range(len(updates)))\n",
    "\n",
    "    return aggregated\n",
    "\n",
    "def fedavg(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    aggregation_weights = [1.0 / len(updates)] * len(updates)\n",
    "    return {k: torch.stack([u[k] for u in updates], 0).mean(0) for k in updates[0].keys()}\n",
    "\n",
    "def median(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    aggregation_weights = [1.0 / len(updates)] * len(updates)\n",
    "    return {k: torch.median(torch.stack([u[k] for u in updates], 0), dim=0)[0] for k in updates[0].keys()}\n",
    "\n",
    "def trimmed_mean(updates, beta=0.1):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    aggregation_weights = [1.0 / len(updates)] * len(updates)\n",
    "    n = len(updates)\n",
    "    trim = int(n * beta)\n",
    "    agg = {}\n",
    "    for k in updates[0].keys():\n",
    "        stacked = torch.stack([u[k] for u in updates], dim=0)\n",
    "        sorted_vals, _ = stacked.sort(dim=0)\n",
    "        trimmed = sorted_vals[trim:n-trim]\n",
    "        agg[k] = trimmed.mean(dim=0)\n",
    "    return agg\n",
    "\n",
    "def krum(updates, f=1):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    aggregation_weights = [1.0 / len(updates)] * len(updates)\n",
    "    scores = []\n",
    "    for i, ui in enumerate(updates):\n",
    "        dists = []\n",
    "        for j, uj in enumerate(updates):\n",
    "            if i != j:\n",
    "                dist = sum(torch.norm(ui[k] - uj[k])**2 for k in ui)\n",
    "                dists.append(dist)\n",
    "        dists.sort()\n",
    "        scores.append((i, sum(dists[:len(updates)-f-2])) if len(dists) > f+2 else (i, float('inf')))\n",
    "    selected = min(scores, key=lambda x: x[1])[0]\n",
    "    return updates[selected]\n",
    "\n",
    "AGGREGATION_FUNCTIONS = {\n",
    "    'FedAVG': fedavg,\n",
    "    'TrimmedMean': trimmed_mean,\n",
    "    'Median': median,\n",
    "    'Krum': krum,\n",
    "    'FLTrust': fltrust,\n",
    "    'FLAME': flame,\n",
    "    'FLCert': flcert,\n",
    "    'FoolsGold': foolsgold,\n",
    "    'RFA': rfa,\n",
    "    'Auror': auror,\n",
    "    'Bulyan': bulyan,\n",
    "    'RFed': rfed\n",
    "}\n",
    "\n",
    "\n",
    "# -------------------\n",
    "# Federated Training Function\n",
    "# -------------------\n",
    "def federated_training(rounds, epochs, client_fraction, attack_types, attack_ratios, defense_method, iid, alpha, device):\n",
    "    cifar_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # normalize CIFAR-10\n",
    "    ])\n",
    "    cifar_train = datasets.CIFAR10(root='./data', train=True, download=True, transform=cifar_transform)\n",
    "    cifar_test = datasets.CIFAR10(root='./data', train=False, download=True, transform=cifar_transform)\n",
    "\n",
    "#     transform = transforms.Compose([transforms.ToTensor()])\n",
    "#     mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "#     mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    #client_data = partition_data(mnist_train, num_clients=100, alpha=alpha, iid=iid)\n",
    "    client_data = partition_data(cifar_train, num_clients=100, alpha=alpha, iid=iid)\n",
    "    test_loader = DataLoader(cifar_test, batch_size=128, shuffle=False)\n",
    "\n",
    "\n",
    "    global malicious_client_ids\n",
    "    malicious_client_ids = []\n",
    "    if attack_types and attack_ratios:\n",
    "        client_data = apply_combined_targeted_attacks_cifar(client_data, attack_types, attack_ratios)\n",
    "\n",
    "    #global_model = SimpleCNN().to(device)\n",
    "    global_model = CIFAR_CNN().to(device)\n",
    "    global_weights = global_model.state_dict()\n",
    "    prev_weights = {k: v.clone() for k, v in global_weights.items()}\n",
    "\n",
    "    results = []\n",
    "    test_loader = DataLoader(cifar_test, batch_size=128, shuffle=False)\n",
    "\n",
    "    for rnd in range(rounds):\n",
    "        global detected_malicious_ids, aggregation_weights\n",
    "        detected_malicious_ids = []\n",
    "        aggregation_weights = []\n",
    "\n",
    "        selected_clients = random.sample(list(client_data.keys()), int(client_fraction * len(client_data)))\n",
    "        local_updates = []\n",
    "        total_train_loss = 0.0\n",
    "        total_train_correct = 0\n",
    "        total_train_samples = 0\n",
    "\n",
    "        for cid in selected_clients:\n",
    "            #local_model = SimpleCNN().to(device)\n",
    "            local_model = CIFAR_CNN().to(device)\n",
    "            local_model.load_state_dict(global_weights)\n",
    "            optimizer = optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "            local_model.train()\n",
    "            for _ in range(epochs):\n",
    "                for x, y in client_data[cid]:\n",
    "                    x, y = x.to(device), y.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    output = local_model(x)\n",
    "                    loss = F.cross_entropy(output, y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    total_train_loss += loss.item() * x.size(0)\n",
    "                    total_train_correct += (output.argmax(1) == y).sum().item()\n",
    "                    total_train_samples += x.size(0)\n",
    "\n",
    "            local_updates.append({k: v.cpu().detach() for k, v in local_model.state_dict().items()})\n",
    "\n",
    "        if defense_method in AGGREGATION_FUNCTIONS:\n",
    "            global_weights = AGGREGATION_FUNCTIONS[defense_method](local_updates)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown defense method: {defense_method}\")\n",
    "\n",
    "        global_model.load_state_dict(global_weights)\n",
    "\n",
    "        acc, prec, rec, f1, fpr, tpr, test_loss = evaluate_model(global_model, test_loader, device)\n",
    "        attack_acc, exclusion_rate = detect_malicious_clients(malicious_client_ids, detected_malicious_ids)\n",
    "        entropy = compute_entropy(aggregation_weights) if aggregation_weights else 0.0\n",
    "        drift = compute_model_drift(prev_weights, global_weights)\n",
    "        prev_weights = {k: v.clone() for k, v in global_weights.items()}\n",
    "\n",
    "        avg_train_loss = total_train_loss / total_train_samples if total_train_samples > 0 else 0.0\n",
    "        avg_train_acc = total_train_correct / total_train_samples if total_train_samples > 0 else 0.0\n",
    "\n",
    "        results.append({\n",
    "            'Round': rnd + 1, 'Accuracy': acc, 'Precision': prec, 'Recall': rec, 'F1': f1,\n",
    "            'FPR': fpr, 'TPR': tpr, 'TestLoss': test_loss,\n",
    "            'TrainLoss': avg_train_loss, 'TrainAccuracy': avg_train_acc,\n",
    "            'AttackDetectAcc': attack_acc, 'ExclusionRate': exclusion_rate,\n",
    "            'Entropy': entropy, 'ModelDrift': drift\n",
    "        })\n",
    "\n",
    "        print(f\"Round {rnd + 1}: Acc={acc:.4f}, Prec={prec:.4f}, Rec={rec:.4f}, F1={f1:.4f}, \"\n",
    "              f\"FPR={fpr:.4f}, TPR={tpr:.4f}, TestLoss={test_loss:.4f}, TrainLoss={avg_train_loss:.4f}, \"\n",
    "              f\"TrainAcc={avg_train_acc:.4f}, AttackDetectAcc={attack_acc:.4f}, \"\n",
    "              f\"ExclusionRate={exclusion_rate:.4f}, Entropy={entropy:.4f}, Drift={drift:.4f}\")\n",
    "\n",
    "    pd.DataFrame(results).to_csv('fl_full_results.csv', index=False)\n",
    "    print(\"Federated training completed. Metrics saved to fl_full_results.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f80cb73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Round 1: Acc=0.0968, Prec=0.0666, Rec=0.0968, F1=0.0357, FPR=0.1004, TPR=0.0968, TestLoss=2.2991, TrainLoss=2.0026, TrainAcc=0.3928, AttackDetectAcc=0.0000, ExclusionRate=0.0000, Entropy=2.3026, Drift=0.0527\n",
      "Round 2: Acc=0.1516, Prec=0.0485, Rec=0.1516, F1=0.0726, FPR=0.0943, TPR=0.1516, TestLoss=2.2981, TrainLoss=1.9678, TrainAcc=0.3624, AttackDetectAcc=0.0000, ExclusionRate=0.0000, Entropy=2.3026, Drift=0.0380\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Start the timer\u001b[39;00m\n\u001b[0;32m      4\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 6\u001b[0m federated_training(\n\u001b[0;32m      7\u001b[0m     rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m      8\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m      9\u001b[0m     client_fraction\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m     10\u001b[0m     attack_types\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_flipping\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_manipulation\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     11\u001b[0m     attack_ratios\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_flipping\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_manipulation\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.3\u001b[39m},\n\u001b[0;32m     12\u001b[0m     defense_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMedian\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     13\u001b[0m     iid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     14\u001b[0m     alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m,\n\u001b[0;32m     15\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     16\u001b[0m )\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# End the timer\u001b[39;00m\n\u001b[0;32m     19\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[1;32mIn[1], line 592\u001b[0m, in \u001b[0;36mfederated_training\u001b[1;34m(rounds, epochs, client_fraction, attack_types, attack_ratios, defense_method, iid, alpha, device)\u001b[0m\n\u001b[0;32m    590\u001b[0m output \u001b[38;5;241m=\u001b[39m local_model(x)\n\u001b[0;32m    591\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(output, y)\n\u001b[1;32m--> 592\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    593\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    595\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "federated_training(\n",
    "    rounds=5,\n",
    "    epochs=2,\n",
    "    client_fraction=0.1,\n",
    "    attack_types=['label_flipping', 'feature_manipulation'],\n",
    "    attack_ratios={'label_flipping': 0.2, 'feature_manipulation': 0.3},\n",
    "    defense_method='Median',\n",
    "    iid=False,\n",
    "    alpha=0.3,\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Compute elapsed time in minutes and seconds\n",
    "elapsed = end_time - start_time\n",
    "minutes = int(elapsed // 60)\n",
    "seconds = int(elapsed % 60)\n",
    "\n",
    "print(f\"\\nTotal Training Time: {minutes} minutes and {seconds} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ed4134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ec9229",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
