{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67e99d6-fac0-4e3e-9cf5-0beaf0d3b45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# CIFAR-100 + ResNet-18 Federated Learning (IID / Dirichlet non-IID)\n",
    "# Attacks: label flipping, feature manipulation, poisoning(trigger)\n",
    "# Defenses/Aggregators: FedAvg, TrimmedMean, Median, Krum, FLTrust, FLAME, FLCert, FoolsGold, RFA, Auror, Bulyan, RFed\n",
    "# -------------------\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Subset, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import resnet18\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "# -------------------\n",
    "# Reproducibility (optional)\n",
    "# -------------------\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# -------------------\n",
    "# ResNet-18 for CIFAR (32x32)\n",
    "# - Modify first conv + remove maxpool\n",
    "# -------------------\n",
    "class ResNet18CIFAR(nn.Module):\n",
    "    def __init__(self, num_classes=100):\n",
    "        super().__init__()\n",
    "        self.model = resnet18(weights=None)  # no pretrained\n",
    "        self.model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.model.maxpool = nn.Identity()\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# -------------------\n",
    "# Data Partition (IID or non-IID using Dirichlet)\n",
    "# Works for CIFAR-100 (100 classes)\n",
    "# -------------------\n",
    "def partition_data(dataset, num_clients=100, alpha=0.5, iid=True, batch_size=64):\n",
    "    n = len(dataset)\n",
    "    all_indices = np.arange(n)\n",
    "\n",
    "    if iid:\n",
    "        np.random.shuffle(all_indices)\n",
    "        splits = np.array_split(all_indices, num_clients)\n",
    "        client_subsets = [Subset(dataset, idxs.tolist()) for idxs in splits]\n",
    "    else:\n",
    "        # Dirichlet non-IID by class\n",
    "        targets = np.array(dataset.targets)\n",
    "        num_classes = int(targets.max() + 1)\n",
    "        idx_by_class = [np.where(targets == c)[0] for c in range(num_classes)]\n",
    "\n",
    "        client_indices = [[] for _ in range(num_clients)]\n",
    "\n",
    "        for c in range(num_classes):\n",
    "            idx_c = idx_by_class[c]\n",
    "            np.random.shuffle(idx_c)\n",
    "\n",
    "            proportions = np.random.dirichlet(np.repeat(alpha, num_clients))\n",
    "            cut_points = (np.cumsum(proportions) * len(idx_c)).astype(int)[:-1]\n",
    "            splits_c = np.split(idx_c, cut_points)\n",
    "\n",
    "            for client_id, split in enumerate(splits_c):\n",
    "                client_indices[client_id].extend(split.tolist())\n",
    "\n",
    "        client_subsets = [Subset(dataset, idxs) for idxs in client_indices]\n",
    "\n",
    "    return {i: DataLoader(client_subsets[i], batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "            for i in range(num_clients)}\n",
    "\n",
    "# -------------------\n",
    "# Attacks\n",
    "# -------------------\n",
    "malicious_client_ids = []\n",
    "\n",
    "def apply_label_flipping_attack(loader, num_classes=100, mode=\"shift1\"):\n",
    "    \"\"\"\n",
    "    mode:\n",
    "      - \"shift1\": y -> (y+1) % C\n",
    "      - \"permute\": random fixed permutation (per call)\n",
    "    \"\"\"\n",
    "    if mode == \"shift1\":\n",
    "        def flip(y): return (y + 1) % num_classes\n",
    "    elif mode == \"permute\":\n",
    "        perm = torch.randperm(num_classes)\n",
    "        def flip(y): return perm[y]\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'shift1' or 'permute'\")\n",
    "\n",
    "    attacked = []\n",
    "    for x, y in loader:\n",
    "        y2 = flip(y)\n",
    "        for i in range(x.size(0)):\n",
    "            attacked.append((x[i].cpu(), y2[i].cpu()))\n",
    "    x_tensor = torch.stack([d[0] for d in attacked])\n",
    "    y_tensor = torch.stack([d[1] for d in attacked]).long()\n",
    "    return DataLoader(TensorDataset(x_tensor, y_tensor), batch_size=loader.batch_size, shuffle=True)\n",
    "\n",
    "def apply_feature_manipulation_attack(loader, noise_std=0.12, mask_prob=0.25):\n",
    "    \"\"\"\n",
    "    Feature manipulation: gaussian noise + random masking (cutout-like).\n",
    "    \"\"\"\n",
    "    attacked = []\n",
    "    for x, y in loader:\n",
    "        x = x.clone()\n",
    "        noise = torch.randn_like(x) * noise_std\n",
    "        x = torch.clamp(x + noise, 0.0, 1.0)\n",
    "\n",
    "        # Random masking\n",
    "        mask = (torch.rand_like(x) > mask_prob).float()\n",
    "        x = x * mask\n",
    "\n",
    "        for i in range(x.size(0)):\n",
    "            attacked.append((x[i].cpu(), y[i].cpu()))\n",
    "    x_tensor = torch.stack([d[0] for d in attacked])\n",
    "    y_tensor = torch.stack([d[1] for d in attacked]).long()\n",
    "    return DataLoader(TensorDataset(x_tensor, y_tensor), batch_size=loader.batch_size, shuffle=True)\n",
    "\n",
    "def apply_poisoning_attack(loader, trigger_size=4, trigger_value=1.0, intensity_shift=0.10):\n",
    "    \"\"\"\n",
    "    Poisoning/backdoor-like: add intensity shift + bottom-right white square trigger.\n",
    "    (Keeps original label like your MNIST code; if you want targeted backdoor, change labels too.)\n",
    "    \"\"\"\n",
    "    attacked = []\n",
    "    for x, y in loader:\n",
    "        x = x.clone()\n",
    "        x = torch.clamp(x + intensity_shift, 0.0, 1.0)\n",
    "\n",
    "        trigger = torch.zeros_like(x)\n",
    "        trigger[:, :, -trigger_size:, -trigger_size:] = trigger_value\n",
    "        x = torch.clamp(x + trigger, 0.0, 1.0)\n",
    "\n",
    "        for i in range(x.size(0)):\n",
    "            attacked.append((x[i].cpu(), y[i].cpu()))\n",
    "    x_tensor = torch.stack([d[0] for d in attacked])\n",
    "    y_tensor = torch.stack([d[1] for d in attacked]).long()\n",
    "    return DataLoader(TensorDataset(x_tensor, y_tensor), batch_size=loader.batch_size, shuffle=True)\n",
    "\n",
    "def apply_combined_attacks(client_data, attack_types, attack_ratios, num_classes=100):\n",
    "    attack_specific_ids = {}\n",
    "    total_clients = list(client_data.keys())\n",
    "    num_clients = len(total_clients)\n",
    "\n",
    "    for attack in attack_types:\n",
    "        count = int(num_clients * attack_ratios.get(attack, 0))\n",
    "        count = max(0, min(count, num_clients))\n",
    "        attack_specific_ids[attack] = random.sample(total_clients, count) if count > 0 else []\n",
    "\n",
    "    malicious_ids = list(set().union(*attack_specific_ids.values())) if attack_specific_ids else []\n",
    "    global malicious_client_ids\n",
    "    malicious_client_ids = malicious_ids\n",
    "\n",
    "    for cid in malicious_ids:\n",
    "        loader = client_data[cid]\n",
    "        if 'label_flipping' in attack_types and cid in attack_specific_ids.get('label_flipping', []):\n",
    "            loader = apply_label_flipping_attack(loader, num_classes=num_classes, mode=\"shift1\")\n",
    "        if 'feature_manipulation' in attack_types and cid in attack_specific_ids.get('feature_manipulation', []):\n",
    "            loader = apply_feature_manipulation_attack(loader, noise_std=0.12, mask_prob=0.25)\n",
    "        if 'poisoning' in attack_types and cid in attack_specific_ids.get('poisoning', []):\n",
    "            loader = apply_poisoning_attack(loader, trigger_size=4, trigger_value=1.0, intensity_shift=0.10)\n",
    "        client_data[cid] = loader\n",
    "\n",
    "    return client_data\n",
    "\n",
    "# -------------------\n",
    "# Evaluation (macro metrics, FPR/TPR averaged across classes)\n",
    "# -------------------\n",
    "def evaluate_model(model, test_loader, device, num_classes=100):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += target.size(0)\n",
    "            y_true.extend(target.cpu().numpy())\n",
    "            y_pred.extend(pred.cpu().numpy())\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(num_classes)))\n",
    "    tp = np.diag(cm)\n",
    "    fp = np.sum(cm, axis=0) - tp\n",
    "    fn = np.sum(cm, axis=1) - tp\n",
    "    tn = np.sum(cm) - (tp + fp + fn)\n",
    "\n",
    "    accuracy = correct / max(total, 1)\n",
    "    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    fpr = np.mean(fp / (fp + tn + 1e-6))\n",
    "    tpr = np.mean(tp / (tp + fn + 1e-6))\n",
    "    test_loss = test_loss / max(total, 1)\n",
    "\n",
    "    return accuracy, precision, recall, f1, fpr, tpr, test_loss\n",
    "\n",
    "# -------------------\n",
    "# Additional Metrics\n",
    "# -------------------\n",
    "def compute_model_drift(prev_weights, new_weights):\n",
    "    keys = list(prev_weights.keys())\n",
    "    return sum(torch.norm(prev_weights[k] - new_weights[k]).item() for k in keys) / max(len(keys), 1)\n",
    "\n",
    "def compute_entropy(weights):\n",
    "    w = np.array(weights, dtype=np.float64)\n",
    "    w = np.clip(w, 1e-12, 1.0)\n",
    "    return float(-np.sum(w * np.log(w)))\n",
    "\n",
    "def detect_malicious_clients(true_ids, detected_ids):\n",
    "    true_set = set(true_ids)\n",
    "    detected_set = set(detected_ids)\n",
    "    tp = len(true_set & detected_set)\n",
    "    fn = len(true_set - detected_set)\n",
    "    fp = len(detected_set - true_set)\n",
    "    attack_detection_rate = tp / (tp + fn + 1e-6)\n",
    "    exclusion_rate = len(detected_set) / (len(true_set | detected_set) + 1e-6)\n",
    "    return attack_detection_rate, exclusion_rate\n",
    "\n",
    "# -------------------\n",
    "# Aggregation / Defenses\n",
    "# NOTE: \"detected_malicious_ids\" are indices within *selected clients list* (0..m-1),\n",
    "# while your \"malicious_client_ids\" are global client IDs (0..N-1). Your detection metric\n",
    "# is NOT valid unless you map indices back to global client IDs. Fix shown in training loop.\n",
    "# -------------------\n",
    "detected_malicious_ids = []\n",
    "aggregation_weights = []\n",
    "\n",
    "def fedavg(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    aggregation_weights = [1.0 / len(updates)] * len(updates)\n",
    "    return {k: torch.stack([u[k] for u in updates], 0).mean(0) for k in updates[0].keys()}\n",
    "\n",
    "def median(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    aggregation_weights = [1.0 / len(updates)] * len(updates)\n",
    "    return {k: torch.median(torch.stack([u[k] for u in updates], 0), dim=0)[0] for k in updates[0].keys()}\n",
    "\n",
    "def trimmed_mean(updates, beta=0.1):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    aggregation_weights = [1.0 / len(updates)] * len(updates)\n",
    "\n",
    "    n = len(updates)\n",
    "    trim = int(n * beta)\n",
    "    if n - 2 * trim <= 0:\n",
    "        return fedavg(updates)\n",
    "\n",
    "    agg = {}\n",
    "    for k in updates[0].keys():\n",
    "        stacked = torch.stack([u[k] for u in updates], dim=0)\n",
    "        sorted_vals, _ = stacked.sort(dim=0)\n",
    "        agg[k] = sorted_vals[trim:n-trim].mean(dim=0)\n",
    "    return agg\n",
    "\n",
    "def krum(updates, f=1):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    aggregation_weights = [1.0 / len(updates)] * len(updates)\n",
    "\n",
    "    n = len(updates)\n",
    "    if n <= f + 2:\n",
    "        return fedavg(updates)\n",
    "\n",
    "    scores = []\n",
    "    for i, ui in enumerate(updates):\n",
    "        dists = []\n",
    "        for j, uj in enumerate(updates):\n",
    "            if i == j:\n",
    "                continue\n",
    "            dist = 0.0\n",
    "            for k in ui.keys():\n",
    "                diff = ui[k] - uj[k]\n",
    "                dist += torch.sum(diff * diff).item()\n",
    "            dists.append(dist)\n",
    "        dists.sort()\n",
    "        scores.append((i, sum(dists[:n - f - 2])))\n",
    "\n",
    "    selected = min(scores, key=lambda x: x[1])[0]\n",
    "    # Mark others as \"malicious\" by exclusion (rough)\n",
    "    detected_malicious_ids = [i for i in range(n) if i != selected]\n",
    "    aggregation_weights = [1.0 if i == selected else 0.0 for i in range(n)]\n",
    "    return updates[selected]\n",
    "\n",
    "def fltrust(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    reference_model = updates[0]  # placeholder reference\n",
    "    scores = []\n",
    "    for update in updates:\n",
    "        sim = 0.0\n",
    "        for k in update.keys():\n",
    "            sim += F.cosine_similarity(update[k].flatten(), reference_model[k].flatten(), dim=0).item()\n",
    "        scores.append(sim / max(len(update), 1))\n",
    "    weights = torch.softmax(torch.tensor(scores), dim=0)\n",
    "    aggregation_weights = weights.tolist()\n",
    "    detected_malicious_ids = [i for i, w in enumerate(weights) if float(w) < 0.01]\n",
    "    return {k: sum(weights[i] * updates[i][k] for i in range(len(updates))) for k in updates[0]}\n",
    "\n",
    "def flame(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    norms = []\n",
    "    for u in updates:\n",
    "        norms.append(sum(torch.norm(u[k]).item() for k in u.keys()))\n",
    "    weights = torch.softmax(-torch.tensor(norms), dim=0)\n",
    "    aggregation_weights = weights.tolist()\n",
    "    return {k: sum(weights[i] * updates[i][k] for i in range(len(updates))) for k in updates[0]}\n",
    "\n",
    "def flcert(updates, threshold=0.5):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    avg = fedavg(updates)\n",
    "    sims = []\n",
    "    for u in updates:\n",
    "        sim = torch.stack([F.cosine_similarity(u[k].flatten(), avg[k].flatten(), dim=0) for k in u.keys()]).mean().item()\n",
    "        sims.append(sim)\n",
    "    keep = [i for i, s in enumerate(sims) if s >= threshold]\n",
    "    detected_malicious_ids = [i for i in range(len(updates)) if i not in keep]\n",
    "    if not keep:\n",
    "        aggregation_weights = [0.0] * len(updates)\n",
    "        return avg\n",
    "    aggregation_weights = [1.0 / len(keep) if i in keep else 0.0 for i in range(len(updates))]\n",
    "    return fedavg([updates[i] for i in keep])\n",
    "\n",
    "def foolsgold(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    grads = [torch.cat([u[k].flatten() for k in sorted(u.keys())]) for u in updates]\n",
    "    n = len(grads)\n",
    "    sim = torch.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                sim[i, j] = F.cosine_similarity(grads[i], grads[j], dim=0)\n",
    "    max_sim = sim.max(dim=1).values\n",
    "    w = 1.0 - max_sim\n",
    "    w = w / (w.sum() + 1e-12)\n",
    "    aggregation_weights = w.tolist()\n",
    "    # crude detection: very low weights\n",
    "    detected_malicious_ids = [i for i, wi in enumerate(w) if float(wi) < 0.01]\n",
    "    return {k: sum(w[i] * updates[i][k] for i in range(n)) for k in updates[0]}\n",
    "\n",
    "def rfa(updates, iters=5, tau=2.0):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "\n",
    "    vecs = [torch.cat([u[k].flatten() for k in sorted(u.keys())]) for u in updates]\n",
    "    stacked = torch.stack(vecs)\n",
    "    median = torch.median(stacked, dim=0)[0]\n",
    "\n",
    "    for _ in range(iters):\n",
    "        d = torch.stack([torch.norm(v - median) for v in vecs]) + 1e-10\n",
    "        w = 1.0 / d\n",
    "        w = w / w.sum()\n",
    "        median = sum(w[i] * vecs[i] for i in range(len(vecs)))\n",
    "\n",
    "    # Detect outliers by distance to median\n",
    "    dists = torch.stack([torch.norm(v - median) for v in vecs]).cpu().numpy()\n",
    "    detected_malicious_ids = [i for i, di in enumerate(dists) if di > tau]\n",
    "    aggregation_weights = [1.0 / len(vecs)] * len(vecs)\n",
    "\n",
    "    # Unflatten\n",
    "    example = updates[0]\n",
    "    agg = {}\n",
    "    ptr = 0\n",
    "    for k in sorted(example.keys()):\n",
    "        numel = example[k].numel()\n",
    "        agg[k] = median[ptr:ptr+numel].view(example[k].shape)\n",
    "        ptr += numel\n",
    "    return agg\n",
    "\n",
    "def auror(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "\n",
    "    flat = [torch.cat([u[k].flatten() for k in sorted(u.keys())]) for u in updates]\n",
    "    n = len(flat)\n",
    "    if n <= 2:\n",
    "        aggregation_weights = [1.0 / n] * n\n",
    "        return fedavg(updates)\n",
    "\n",
    "    dist = torch.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            d = torch.norm(flat[i] - flat[j]).item()\n",
    "            dist[i, j] = dist[j, i] = d\n",
    "\n",
    "    k = max(1, n // 10)\n",
    "    scores = []\n",
    "    for i in range(n):\n",
    "        sorted_d = torch.sort(dist[i])[0]\n",
    "        scores.append(sorted_d[1:k+1].mean().item())\n",
    "\n",
    "    keep = sorted(range(n), key=lambda i: scores[i])[: max(1, n // 2)]\n",
    "    detected_malicious_ids = [i for i in range(n) if i not in keep]\n",
    "    aggregation_weights = [1.0 / len(keep) if i in keep else 0.0 for i in range(n)]\n",
    "    return fedavg([updates[i] for i in keep])\n",
    "\n",
    "def bulyan(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "    n = len(updates)\n",
    "    if n < 4:\n",
    "        aggregation_weights = [1.0 / n] * n\n",
    "        return fedavg(updates)\n",
    "\n",
    "    # Choose f as 10% of participants (same style as your MNIST code)\n",
    "    f = max(1, n // 10)\n",
    "\n",
    "    # Pairwise distances\n",
    "    dist = torch.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            d = 0.0\n",
    "            for k in updates[i].keys():\n",
    "                diff = updates[i][k] - updates[j][k]\n",
    "                d += torch.sum(diff * diff).item()\n",
    "            dist[i, j] = dist[j, i] = d\n",
    "\n",
    "    # Krum scores\n",
    "    candidates = []\n",
    "    m = max(1, n - f - 2)\n",
    "    for i in range(n):\n",
    "        dists = sorted([dist[i, j].item() for j in range(n) if j != i])\n",
    "        candidates.append((i, sum(dists[:m])))\n",
    "    candidates.sort(key=lambda x: x[1])\n",
    "\n",
    "    # Select n - 2f\n",
    "    keep = [i for i, _ in candidates[: max(1, n - 2 * f)]]\n",
    "    detected_malicious_ids = [i for i in range(n) if i not in keep]\n",
    "    aggregation_weights = [1.0 / len(keep) if i in keep else 0.0 for i in range(n)]\n",
    "\n",
    "    # Then trimmed mean on kept set\n",
    "    return trimmed_mean([updates[i] for i in keep], beta=0.1)\n",
    "\n",
    "def rfed(updates):\n",
    "    global detected_malicious_ids, aggregation_weights\n",
    "    detected_malicious_ids = []\n",
    "\n",
    "    flat = [torch.cat([u[k].flatten() for k in sorted(u.keys())]) for u in updates]\n",
    "    stacked = torch.stack(flat)\n",
    "    var = torch.var(stacked, dim=0) + 1e-6\n",
    "    mean = stacked.mean(dim=0)\n",
    "\n",
    "    dists = []\n",
    "    for v in flat:\n",
    "        diff = v - mean\n",
    "        d = (diff * diff / var).mean().item()\n",
    "        dists.append(d)\n",
    "\n",
    "    w = torch.softmax(-torch.tensor(dists), dim=0)\n",
    "    aggregation_weights = w.tolist()\n",
    "\n",
    "    agg = {}\n",
    "    for k in updates[0].keys():\n",
    "        agg[k] = sum(w[i] * updates[i][k] for i in range(len(updates)))\n",
    "    return agg\n",
    "\n",
    "AGGREGATION_FUNCTIONS = {\n",
    "    'FedAVG': fedavg,\n",
    "    'TrimmedMean': trimmed_mean,\n",
    "    'Median': median,\n",
    "    'Krum': krum,\n",
    "    'FLTrust': fltrust,\n",
    "    'FLAME': flame,\n",
    "    'FLCert': flcert,\n",
    "    'FoolsGold': foolsgold,\n",
    "    'RFA': rfa,\n",
    "    'Auror': auror,\n",
    "    'Bulyan': bulyan,\n",
    "    'RFed': rfed\n",
    "}\n",
    "\n",
    "# -------------------\n",
    "# Federated Training (CIFAR-100)\n",
    "# IMPORTANT FIX: map detected indices -> global client IDs (otherwise detection metric is wrong)\n",
    "# -------------------\n",
    "def federated_training_cifar100(\n",
    "    rounds=20,\n",
    "    epochs=1,\n",
    "    client_fraction=0.1,\n",
    "    attack_types=None,\n",
    "    attack_ratios=None,\n",
    "    defense_method='FedAVG',\n",
    "    iid=True,\n",
    "    alpha=0.5,\n",
    "    num_clients=100,\n",
    "    batch_size=64,\n",
    "    lr=0.01,\n",
    "    momentum=0.9,\n",
    "    weight_decay=5e-4,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=42\n",
    "):\n",
    "    set_seed(seed)\n",
    "\n",
    "    # CIFAR-100 normalization (standard)\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5071, 0.4867, 0.4408),\n",
    "                             (0.2675, 0.2565, 0.2761)),\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5071, 0.4867, 0.4408),\n",
    "                             (0.2675, 0.2565, 0.2761)),\n",
    "    ])\n",
    "\n",
    "    cifar_train = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
    "    cifar_test = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "    client_data = partition_data(cifar_train, num_clients=num_clients, alpha=alpha, iid=iid, batch_size=batch_size)\n",
    "\n",
    "    global malicious_client_ids\n",
    "    malicious_client_ids = []\n",
    "    if attack_types and attack_ratios:\n",
    "        client_data = apply_combined_attacks(client_data, attack_types, attack_ratios, num_classes=100)\n",
    "\n",
    "    global_model = ResNet18CIFAR(num_classes=100).to(device)\n",
    "    global_weights = {k: v.detach().cpu().clone() for k, v in global_model.state_dict().items()}\n",
    "    prev_weights = {k: v.clone() for k, v in global_weights.items()}\n",
    "\n",
    "    test_loader = DataLoader(cifar_test, batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for rnd in range(rounds):\n",
    "        global detected_malicious_ids, aggregation_weights\n",
    "        detected_malicious_ids = []\n",
    "        aggregation_weights = []\n",
    "\n",
    "        m = max(1, int(client_fraction * num_clients))\n",
    "        selected_clients = random.sample(list(client_data.keys()), m)\n",
    "\n",
    "        local_updates = []\n",
    "        total_train_loss = 0.0\n",
    "        total_train_correct = 0\n",
    "        total_train_samples = 0\n",
    "\n",
    "        for global_cid in selected_clients:\n",
    "            local_model = ResNet18CIFAR(num_classes=100).to(device)\n",
    "            local_model.load_state_dict(global_weights, strict=True)\n",
    "            local_model.train()\n",
    "\n",
    "            optimizer = optim.SGD(local_model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "            for _ in range(epochs):\n",
    "                for x, y in client_data[global_cid]:\n",
    "                    x, y = x.to(device), y.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    out = local_model(x)\n",
    "                    loss = F.cross_entropy(out, y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    total_train_loss += loss.item() * x.size(0)\n",
    "                    total_train_correct += (out.argmax(1) == y).sum().item()\n",
    "                    total_train_samples += x.size(0)\n",
    "\n",
    "            local_updates.append({k: v.detach().cpu() for k, v in local_model.state_dict().items()})\n",
    "\n",
    "        if defense_method not in AGGREGATION_FUNCTIONS:\n",
    "            raise ValueError(f\"Unknown defense method: {defense_method}\")\n",
    "\n",
    "        # Aggregate\n",
    "        new_global_weights = AGGREGATION_FUNCTIONS[defense_method](local_updates)\n",
    "\n",
    "        # Load to global model\n",
    "        global_weights = {k: v.detach().cpu().clone() for k, v in new_global_weights.items()}\n",
    "        global_model.load_state_dict(global_weights, strict=True)\n",
    "\n",
    "        # Evaluate\n",
    "        acc, prec, rec, f1, fpr, tpr, test_loss = evaluate_model(global_model, test_loader, device, num_classes=100)\n",
    "\n",
    "        # FIX: detected_malicious_ids are indices in [0..m-1] -> map to global client IDs\n",
    "        detected_global_ids = [selected_clients[i] for i in detected_malicious_ids if 0 <= i < len(selected_clients)]\n",
    "\n",
    "        attack_acc, exclusion_rate = detect_malicious_clients(malicious_client_ids, detected_global_ids)\n",
    "        entropy = compute_entropy(aggregation_weights) if aggregation_weights else 0.0\n",
    "        drift = compute_model_drift(prev_weights, global_weights)\n",
    "        prev_weights = {k: v.clone() for k, v in global_weights.items()}\n",
    "\n",
    "        avg_train_loss = total_train_loss / max(total_train_samples, 1)\n",
    "        avg_train_acc = total_train_correct / max(total_train_samples, 1)\n",
    "\n",
    "        row = {\n",
    "            'Round': rnd + 1,\n",
    "            'Accuracy': acc, 'Precision': prec, 'Recall': rec, 'F1': f1,\n",
    "            'FPR': fpr, 'TPR': tpr, 'TestLoss': test_loss,\n",
    "            'TrainLoss': avg_train_loss, 'TrainAccuracy': avg_train_acc,\n",
    "            'AttackDetectAcc': attack_acc, 'ExclusionRate': exclusion_rate,\n",
    "            'Entropy': entropy, 'ModelDrift': drift\n",
    "        }\n",
    "        results.append(row)\n",
    "\n",
    "        print(\n",
    "            f\"Round {rnd+1:03d} | \"\n",
    "            f\"Acc={acc:.4f} F1={f1:.4f} TestLoss={test_loss:.4f} | \"\n",
    "            f\"TrainAcc={avg_train_acc:.4f} TrainLoss={avg_train_loss:.4f} | \"\n",
    "            f\"DetectAcc={attack_acc:.4f} ExclRate={exclusion_rate:.4f} | \"\n",
    "            f\"Entropy={entropy:.4f} Drift={drift:.4f}\"\n",
    "        )\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv('fl_cifar100_resnet18_results.csv', index=False)\n",
    "    print(\"Done. Saved: fl_cifar100_resnet18_results.csv\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# -------------------\n",
    "# Example usage\n",
    "# -------------------\n",
    "if __name__ == \"__main__\":\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # Example: non-IID, composite attacks, Bulyan defense\n",
    "    df = federated_training_cifar100(\n",
    "        rounds=10,\n",
    "        epochs=1,\n",
    "        client_fraction=0.1,\n",
    "        attack_types=['label_flipping', 'feature_manipulation', 'poisoning'],\n",
    "        attack_ratios={'label_flipping': 0.2, 'feature_manipulation': 0.2, 'poisoning': 0.2},\n",
    "        defense_method='Bulyan',\n",
    "        iid=False,\n",
    "        alpha=0.3,\n",
    "        num_clients=100,\n",
    "        batch_size=64,\n",
    "        lr=0.01,\n",
    "        device=device,\n",
    "        seed=42\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
